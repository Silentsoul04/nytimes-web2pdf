<div id="app">

<div>

<div>

<div>

<div class="NYTAppHideMasthead css-1q2w90k e1suatyy0">

<div class="section css-ui9rw0 e1suatyy2">

<div class="css-eph4ug er09x8g0">

<div class="css-6n7j50">

</div>

<span class="css-1dv1kvn">Sections</span>

<div class="css-10488qs">

<span class="css-1dv1kvn">SEARCH</span>

</div>

[Skip to content](#site-content)[Skip to site
index](#site-index)

</div>

<div id="masthead-section-label" class="css-1wr3we4 eaxe0e00">

[Technology](https://www.nytimes.com/section/technology)

</div>

<div class="css-10698na e1huz5gh0">

</div>

</div>

<div id="masthead-bar-one" class="section hasLinks css-15hmgas e1csuq9d3">

<div class="css-uqyvli e1csuq9d0">

</div>

<div class="css-1uqjmks e1csuq9d1">

</div>

<div class="css-9e9ivx">

[](https://myaccount.nytimes.com/auth/login?response_type=cookie&client_id=vi)

</div>

<div class="css-1bvtpon e1csuq9d2">

[Today’s
Paper](https://www.nytimes.com/section/todayspaper)

</div>

</div>

</div>

</div>

<div data-aria-hidden="false">

<div id="site-content" data-role="main">

<div>

<div class="css-1aor85t" style="opacity:0.000000001;z-index:-1;visibility:hidden">

<div class="css-1hqnpie">

<div class="css-epjblv">

<span class="css-17xtcya">[Technology](/section/technology)</span><span class="css-x15j1o">|</span><span class="css-fwqvlz">Facial
Recognition Is Accurate, if You’re a White
Guy</span>

</div>

<div class="css-k008qs">

<div class="css-1iwv8en">

<span class="css-18z7m18"></span>

<div>

</div>

</div>

<span class="css-1n6z4y">https://nyti.ms/2BNurVq</span>

<div class="css-1705lsu">

<div class="css-4xjgmj">

<div class="css-4skfbu" data-role="toolbar" data-aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count" data-testid="share-tools">

  - 
  - 
  - 
  - 
    
    <div class="css-6n7j50">
    
    </div>

  - 

</div>

</div>

</div>

</div>

</div>

</div>

<div id="NYT_TOP_BANNER_REGION" class="css-13pd83m">

</div>

<div id="top-wrapper" class="css-1sy8kpn">

<div id="top-slug" class="css-l9onyx">

Advertisement

</div>

[Continue reading the main
story](#after-top)

<div class="ad top-wrapper" style="text-align:center;height:100%;display:block;min-height:250px">

<div id="top" class="place-ad" data-position="top" data-size-key="top">

</div>

</div>

<div id="after-top">

</div>

</div>

<div id="sponsor-wrapper" class="css-1hyfx7x">

<div id="sponsor-slug" class="css-19vbshk">

Supported by

</div>

[Continue reading the main
story](#after-sponsor)

<div id="sponsor" class="ad sponsor-wrapper" style="text-align:center;height:100%;display:block">

</div>

<div id="after-sponsor">

</div>

</div>

<div class="css-1vkm6nb ehdk2mb0">

# Facial Recognition Is Accurate, if You’re a White Guy

</div>

<div class="css-xt80pu e12qa4dv0">

<div class="css-18e8msd">

<div class="css-vp77d3 epjyd6m0">

<div class="css-1baulvz">

By [<span class="css-1baulvz last-byline" itemprop="name">Steve
Lohr</span>](http://www.nytimes.com/by/steve-lohr)

</div>

</div>

  - Feb. 9,
    2018

  - 
    
    <div class="css-4xjgmj">
    
    <div class="css-d8bdto" data-role="toolbar" data-aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count" data-testid="share-tools">
    
      - 
      - 
      - 
      - 
        
        <div class="css-6n7j50">
        
        </div>
    
      - 
    
    </div>
    
    </div>

</div>

</div>

<div class="section meteredContent css-1r7ky0e" name="articleBody" itemprop="articleBody">

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Facial recognition technology is improving by leaps and bounds. Some
commercial software can now tell the gender of a person in a photograph.

When the person in the photo is a white man, the software is right 99
percent of the time.

But the darker the skin, the more errors arise — up to nearly 35 percent
for images of darker skinned women, according to a new study that breaks
fresh ground by measuring how the technology works on people of
different races and gender.

These disparate results, calculated by Joy Buolamwini, a researcher at
the M.I.T. Media Lab, show how some of the biases in the real world can
seep into artificial intelligence, the computer systems that inform
facial
recognition.

</div>

</div>

<div style="max-width:100%;margin:0 auto">

<div class="css-17dprlf" data-id="100000005728381" data-slug="ai-faces-photo-grid" style="max-width:600px">

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

In modern artificial intelligence, data rules. A.I. software is only as
smart as the data used to train it. If there are many more white men
than black women in the system, it will be worse at identifying the
black women.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

One widely used facial-recognition data set was estimated to be more
than 75 percent male and more than 80 percent white, according to
another research study.

The new study also raises broader questions of fairness and
accountability in artificial intelligence at a time when investment in
and adoption of the technology is racing ahead.

Today, facial recognition software is being deployed by companies in
various ways, including to help target product pitches based on social
media profile pictures. But companies are also experimenting with face
identification and other A.I. technology as an ingredient in automated
decisions with higher stakes like hiring and lending.

Researchers at the Georgetown Law School [estimated that 117 million
American adults](https://www.perpetuallineup.org/) are in face
recognition networks used by law enforcement — and that African
Americans were most likely to be singled out, because they were
disproportionately represented in mug-shot databases.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Facial recognition technology is lightly regulated so far.

“This is the right time to be addressing how these A.I. systems work and
where they fail — to make them socially accountable,” said Suresh
Venkatasubramanian, a professor of computer science at the University of
Utah.

Until now, there was anecdotal evidence of computer vision miscues, and
occasionally in ways that suggested discrimination. In 2015, for
example, [Google had to
apologize](https://bits.blogs.nytimes.com/2015/07/01/google-photos-mistakenly-labels-black-people-gorillas/)
after its image-recognition photo app initially labeled African
Americans as “gorillas.”

Sorelle Friedler, a computer scientist at Haverford College and a
reviewing editor on Ms. Buolamwini’s research paper, said experts had
long suspected that facial recognition software performed differently on
different populations.

“But this is the first work I’m aware of that shows that empirically,”
Ms. Friedler said.

Ms. Buolamwini, a young African-American computer scientist, experienced
the bias of facial recognition firsthand. When she was an undergraduate
at the Georgia Institute of Technology, programs would work well on her
white friends, she said, but not recognize her face at all. She figured
it was a flaw that would surely be fixed before long.

</div>

</div>

<div class="css-79elbk" data-testid="photoviewer-wrapper">

<div class="css-z3e15g" data-testid="photoviewer-wrapper-hidden">

</div>

<div class="css-1a48zt4 ehw59r15" data-testid="photoviewer-children">

![<span class="css-16f3y1r e13ogyst0" data-aria-hidden="true">Joy
Buolamwini, a researcher at the M.I.T. Media Lab, has emerged as an
advocate in the new field of “algorithmic
accountability.”</span><span class="css-cnj6d5 e1z0qqy90" itemprop="copyrightHolder"><span class="css-1ly73wi e1tej78p0">Credit...</span><span>Tony
Luong for The New York
Times</span></span>](https://static01.nyt.com/images/2018/02/10/business/12FACES-2/12FACES-2-articleLarge-v2.jpg?quality=75&auto=webp&disable=upscale)

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

But a few years later, after joining the M.I.T. Media Lab, she ran into
the missing-face problem again. Only when she put on a white mask did
the software recognize hers as a face.

By then, face recognition software was increasingly moving out of the
lab and into the mainstream.

“O.K., this is serious,” she recalled deciding then. “Time to do
something.”

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

So she turned her attention to fighting the bias built into digital
technology. Now 28 and a doctoral student, after studying as a Rhodes
scholar and a Fulbright fellow, she is an advocate in the new field of
“algorithmic accountability,” which seeks to make automated decisions
more transparent, explainable and fair.

Her [short TED
Talk](https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms)
on coded bias has been viewed more than 940,000 times, and she founded
the [Algorithmic Justice League](https://www.ajlunited.org/), a project
to raise awareness of the issue.

In her newly [published
paper](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf),
which will be presented at [a conference](https://fatconference.org/)
this month, Ms. Buolamwini studied the performance of three leading face
recognition systems — by Microsoft, IBM and Megvii of China — by
classifying how well they could guess the gender of people with
different skin tones. These companies were selected because they offered
gender classification features in their facial analysis software — and
their code was publicly available for testing.

She found them all wanting.

To test the commercial systems, Ms. Buolamwini built a data set of 1,270
faces, using faces of lawmakers from countries with a high percentage of
women in office. The sources included three African nations with
predominantly dark-skinned populations, and three Nordic countries with
mainly light-skinned residents.

The African and Nordic faces were scored according to a six-point
labeling system used by dermatologists to classify skin types. The
medical classifications were determined to be more objective and precise
than race.

Then, each company’s software was tested on the curated data, crafted
for gender balance and a range of skin tones. The results varied
somewhat. Microsoft’s error rate for darker-skinned women was 21
percent, while IBM’s and Megvii’s rates were nearly 35 percent. They all
had error rates below 1 percent for light-skinned males.

Ms. Buolamwini shared the research results with each of the companies.
IBM said in a statement to her that the company had steadily improved
its facial analysis software and was “deeply committed” to “unbiased”
and “transparent” services. This month, the company said, it will roll
out an improved service with a nearly 10-fold increase in accuracy on
darker-skinned women.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Microsoft said that it had “already taken steps to improve the accuracy
of our facial recognition technology” and that it was investing in
research “to recognize, understand and remove bias.”

Ms. Buolamwini’s co-author on her paper is Timnit Gebru, who described
her role as an adviser. Ms. Gebru is a scientist at Microsoft Research,
working on its [Fairness Accountability Transparency and Ethics in
A.I.](https://www.microsoft.com/en-us/research/group/fate/)group.

</div>

</div>

<div class="css-79elbk" data-testid="photoviewer-wrapper">

<div class="css-z3e15g" data-testid="photoviewer-wrapper-hidden">

</div>

<div class="css-1a48zt4 ehw59r15" data-testid="photoviewer-children">

<div class="css-1xdhyk6 erfvjey0">

<span class="css-1ly73wi e1tej78p0">Image</span>

<div class="css-zjzyr8">

<div data-testid="lazyimage-container" style="height:290px">

</div>

</div>

</div>

<span class="css-16f3y1r e13ogyst0" data-aria-hidden="true">Timnit
Gebru, a scientist at Microsoft Research, was a co-author of the paper
that studied facial recognition
software.</span><span class="css-cnj6d5 e1z0qqy90" itemprop="copyrightHolder"><span class="css-1ly73wi e1tej78p0">Credit...</span><span>Cody
O'Loughlin for The New York Times</span></span>

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Megvii, whose Face++ software is widely used for identification in
online payment and ride-sharing services in China, did not reply to
several requests for comment, Ms. Buolamwini said.

Ms. Buolamwini is releasing her data set for others to build upon. She
describes her research as “a starting point, very much a first step”
toward solutions.

Ms. Buolamwini is taking further steps in the technical community and
beyond. She is working with the [Institute of Electrical and Electronics
Engineers](https://www.ieee.org/index.html), a large professional
organization in computing, to set up a group to create standards for
accountability and transparency in facial analysis software.

She meets regularly with other academics, public policy groups and
philanthropies that are concerned about the impact of artificial
intelligence. Darren Walker, president of the [Ford
Foundation](https://www.fordfoundation.org/), said that the new
technology could be a “platform for opportunity,” but that it would not
happen if it replicated and amplified bias and discrimination of the
past.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

“There is a battle going on for fairness, inclusion and justice in the
digital world,” Mr. Walker said.

Part of the challenge, scientists say, is that there is so little
diversity within the A.I. community.  

“We’d have a lot more introspection and accountability in the field of
A.I. if we had more people like Joy,” said Cathy O’Neil, a data
scientist and author of “Weapons of Math Destruction.”

Technology, Ms. Buolamwini said, should be more attuned to the people
who use it and the people it’s used on.

“You can’t have ethical A.I. that’s not inclusive,” she said. “And
whoever is creating the technology is setting the standards.”

</div>

</div>

</div>

<div>

</div>

<div>

</div>

<div>

</div>

<div>

<div id="bottom-wrapper" class="css-1ede5it">

<div id="bottom-slug" class="css-l9onyx">

Advertisement

</div>

[Continue reading the main
story](#after-bottom)

<div id="bottom" class="ad bottom-wrapper" style="text-align:center;height:100%;display:block;min-height:90px">

</div>

<div id="after-bottom">

</div>

</div>

</div>

</div>

</div>

## Site Index

<div>

</div>

## Site Information Navigation

  - [© <span>2020</span> <span>The New York Times
    Company</span>](https://help.nytimes.com/hc/en-us/articles/115014792127-Copyright-notice)

<!-- end list -->

  - [NYTCo](https://www.nytco.com/)
  - [Contact
    Us](https://help.nytimes.com/hc/en-us/articles/115015385887-Contact-Us)
  - [Work with us](https://www.nytco.com/careers/)
  - [Advertise](https://nytmediakit.com/)
  - [T Brand Studio](http://www.tbrandstudio.com/)
  - [Your Ad
    Choices](https://www.nytimes.com/privacy/cookie-policy#how-do-i-manage-trackers)
  - [Privacy](https://www.nytimes.com/privacy)
  - [Terms of
    Service](https://help.nytimes.com/hc/en-us/articles/115014893428-Terms-of-service)
  - [Terms of
    Sale](https://help.nytimes.com/hc/en-us/articles/115014893968-Terms-of-sale)
  - [Site
    Map](https://spiderbites.nytimes.com)
  - [Help](https://help.nytimes.com/hc/en-us)
  - [Subscriptions](https://www.nytimes.com/subscription?campaignId=37WXW)

</div>

</div>

</div>

</div>
