<div id="app">

<div>

<div>

<div>

<div class="NYTAppHideMasthead css-1q2w90k e1suatyy0">

<div class="section css-ui9rw0 e1suatyy2">

<div class="css-eph4ug er09x8g0">

<div class="css-6n7j50">

</div>

<span class="css-1dv1kvn">Sections</span>

<div class="css-10488qs">

<span class="css-1dv1kvn">SEARCH</span>

</div>

[Skip to content](#site-content)[Skip to site index](#site-index)

</div>

<div id="masthead-section-label" class="css-1wr3we4 eaxe0e00">

[Technology](https://www.nytimes.com/section/technology)

</div>

<div class="css-10698na e1huz5gh0">

</div>

</div>

<div id="masthead-bar-one" class="section hasLinks css-15hmgas e1csuq9d3">

<div class="css-uqyvli e1csuq9d0">

</div>

<div class="css-1uqjmks e1csuq9d1">

</div>

<div class="css-9e9ivx">

[](https://myaccount.nytimes.com/auth/login?response_type=cookie&client_id=vi)

</div>

<div class="css-1bvtpon e1csuq9d2">

[Today’s Paper](https://www.nytimes.com/section/todayspaper)

</div>

</div>

</div>

</div>

<div data-aria-hidden="false">

<div id="site-content" role="main">

<div>

<div class="css-1aor85t" style="opacity:0.000000001;z-index:-1;visibility:hidden">

<div class="css-1hqnpie">

<div class="css-epjblv">

<span class="css-17xtcya">[Technology](/section/technology)</span><span class="css-x15j1o">|</span><span class="css-fwqvlz">Here
Come the Fake Videos, Too</span>

</div>

<div class="css-k008qs">

<div class="css-1iwv8en">

<span class="css-18z7m18"></span>

<div>

</div>

</div>

<span class="css-1n6z4y">https://nyti.ms/2FVUA3O</span>

<div class="css-1705lsu">

<div class="css-4xjgmj">

<div class="css-4skfbu" role="toolbar" data-aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count" data-testid="share-tools">

  - 
  - 
  - 
  - 
    
    <div class="css-6n7j50">
    
    </div>

  - 
  - 

</div>

</div>

</div>

</div>

</div>

</div>

<div id="NYT_TOP_BANNER_REGION" class="css-13pd83m">

</div>

<div id="top-wrapper" class="css-1sy8kpn">

<div id="top-slug" class="css-l9onyx">

Advertisement

</div>

[Continue reading the main story](#after-top)

<div class="ad top-wrapper" style="text-align:center;height:100%;display:block;min-height:250px">

<div id="top" class="place-ad" data-position="top" data-size-key="top">

</div>

</div>

<div id="after-top">

</div>

</div>

<div>

<div id="sponsor-wrapper" class="css-1hyfx7x">

<div id="sponsor-slug" class="css-19vbshk">

Supported by

</div>

[Continue reading the main story](#after-sponsor)

<div id="sponsor" class="ad sponsor-wrapper" style="text-align:center;height:100%;display:block">

</div>

<div id="after-sponsor">

</div>

</div>

<div class="css-186x18t">

</div>

<div class="css-1vkm6nb ehdk2mb0">

# Here Come the Fake Videos, Too

</div>

Artificial intelligence video tools make it relatively easy to put one
person’s face on another person’s body with few traces of manipulation.
I tried it on myself. What could go wrong?

<div class="css-18e8msd">

<div class="css-vp77d3 epjyd6m0">

<div class="css-hus3qt ey68jwv0" data-aria-hidden="true">

[![Kevin
Roose](https://static01.nyt.com/images/2018/02/20/multimedia/author-kevin-roose/author-kevin-roose-thumbLarge.jpg
"Kevin Roose")](https://www.nytimes.com/by/kevin-roose)

</div>

<div class="css-1baulvz">

By [<span class="css-1baulvz last-byline" itemprop="name">Kevin
Roose</span>](https://www.nytimes.com/by/kevin-roose)

</div>

</div>

  - March 4, 2018

  - 
    
    <div class="css-4xjgmj">
    
    <div class="css-d8bdto" role="toolbar" data-aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count" data-testid="share-tools">
    
      - 
      - 
      - 
      - 
        
        <div class="css-6n7j50">
        
        </div>
    
      - 
      - 
    
    </div>
    
    </div>

</div>

<div class="css-mdjrty">

[Leer en
español](https://www.nytimes.com/es/2018/03/07/noticias-falsas-videomontajes-deepfake-fakeapp/ "Read in Spanish")

</div>

</div>

<div class="section meteredContent css-1r7ky0e" name="articleBody" itemprop="articleBody">

<div style="max-width:100%;margin:0 auto">

<div id="100000005774492" class="css-17dprlf" data-slug="roose-deepfake-top" style="max-width:945px">

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

The scene opened on a room with a red sofa, a potted plant and the kind
of bland modern art you’d see on a therapist’s wall.

In the room was Michelle Obama, or someone who looked exactly like her.
Wearing a low-cut top with a black bra visible underneath, she writhed
lustily for the camera and flashed her unmistakable smile.

Then, the former first lady’s doppelgänger began to strip.

The video, which appeared on the online forum Reddit, was what’s known
as a “deepfake” — an ultrarealistic fake video made with artificial
intelligence software. It was created using a program called FakeApp,
which superimposed Mrs. Obama’s face onto the body of a pornographic
film actress. The hybrid was uncanny — if you didn’t know better, you
might have thought it was really her.

Until recently, realistic computer-generated video was a laborious
pursuit available only to big-budget Hollywood productions or
cutting-edge researchers. Social media apps like Snapchat include some
rudimentary face-morphing technology.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

But in recent months, a community of hobbyists has begun experimenting
with more powerful tools, including FakeApp — a program that was built
by an anonymous developer using open-source software written by Google.
FakeApp makes it free and relatively easy to create realistic face swaps
and leave few traces of manipulation. Since a version of the app
appeared on Reddit in January, it has been downloaded more than 120,000
times, according to its creator.

Deepfakes are one of the newest forms of digital media manipulation, and
one of the most obviously mischief-prone. It’s not hard to imagine this
technology’s being used to smear politicians, create counterfeit revenge
porn or frame people for crimes. Lawmakers have already [begun to
worry](http://thehill.com/policy/technology/374320-lawmakers-worry-about-rise-of-fake-video-technology)
about how deepfakes could be used for political sabotage and
propaganda.<span class="css-8l6xbc evw5hdy0"> </span>

Even on morally lax sites like Reddit, deepfakes have raised eyebrows.
Recently, FakeApp set off a panic after Motherboard, the technology
site,
[reported](https://motherboard.vice.com/en_us/article/gydydm/gal-gadot-fake-ai-porn)
that people were using it to create pornographic deepfakes of
celebrities. Pornhub, Twitter and other sites quickly [banned the
videos](http://variety.com/2018/digital/news/reddit-twitter-deepfake-ban-1202690627/),
and Reddit closed a handful of deepfake groups, including one with
nearly 100,000 members.

Before the Reddit deepfake groups were closed, they hosted a mixture of
users trading video-editing tips and showing off their latest forgeries.
A post titled “3D face reconstruction for additional angles” sat next to
videos with titles like “(Not) Olivia Wilde playing with herself.”

Some users on Reddit defended deepfakes and blamed the media for
overhyping their potential for harm. Others moved their videos to
alternative platforms, rightly anticipating that Reddit would crack down
under its rules against nonconsensual pornography. And a few expressed
moral qualms about putting the technology into the world.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Then, they kept making more.

The deepfake creator community is now in the internet’s shadows. But
while out in the open, it gave an unsettling peek into the future.

“This is turning into an episode of Black Mirror,” wrote one Reddit
user. The post raised the ontological questions at the heart of the
deepfake debate: Does a naked image of Person A become a naked image of
Person B if Person B’s face is superimposed in a seamless and
untraceable way? In a broader sense, on the internet, what is the
difference between representation and reality?

The user then signed off with a shrug: “Godspeed rebels.”

## Making Deepfakes

After lurking for several weeks in Reddit’s deepfake community, I
decided to see how easy it was to create a (safe for work,
nonpornographic) deepfake using my own face.

I started by downloading FakeApp and enlisting two technical experts to
help me. The first was Mark McKeague, a colleague in The New York
Times’s research and development department. The second was a deepfake
creator I found through Reddit, who goes by the nickname Derpfakes.

Because of the controversial nature of deepfakes, Derpfakes would not
give his or her real name. Derpfakes started [posting deepfake
videos](https://www.youtube.com/channel/UCUix6Sk2MZkVOr5PWQrtH1g) on
YouTube a few weeks ago, specializing in humorous offerings like Nicolas
Cage playing Superman. The account has also posted some how-to videos on
deepfake creation.

What I learned is that making a deepfake isn’t simple. But it’s not
rocket science, either.

The first step is to find, or rent, a moderately powerful computer.
FakeApp uses a suite of machine learning tools called TensorFlow, which
was developed by Google’s A.I. division and released to the public in
2015. The software teaches itself to perform image-recognition tasks
[through trial and
error](https://www.nytimes.com/interactive/2018/01/02/technology/ai-generated-photos.html).
The more processing power on hand, the faster it works.

To get more speed, Mark and I used a remote server rented through Google
Cloud Platform. It provided enough processing power to cut the time
frame down to hours, rather than the days or weeks it might take on my
laptop.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Once Mark set up the remote server and loaded FakeApp on it, we were on
to the next step: data collection.

Picking the right source data is crucial. Short video clips are easier
to manipulate than long clips, and scenes shot at a single angle produce
better results than scenes with multiple angles. Genetics also help. The
more the faces resemble each other, the better.

I’m a brown-haired white man with a short beard, so Mark and I decided
to try several other brown-haired, stubbled white guys. We started with
Ryan Gosling. (Aim high, right?) I also sent Derpfakes, my outsourced
Reddit expert, several video options to choose from.

Next, we took several hundred photos of my face, and gathered images of
Mr. Gosling’s face using a clip from a recent TV appearance. FakeApp
uses these images to train the deep learning model and teach it to
emulate our facial expressions.<span class="css-8l6xbc evw5hdy0">
</span>

To get the broadest photo set possible, I twisted my head at different
angles, making as many different faces as I could.

</div>

</div>

<div style="max-width:100%;margin:0 auto">

<div id="100000005776340" class="css-17dprlf" data-slug="roose-deepfake-loop-1" style="max-width:945px">

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Mark then used a program to crop those images down, isolating just our
faces, and manually deleted any blurred or badly cropped photos. He then
fed the frames into FakeApp. In all, we used 417 photos of me, and 1,113
of Mr. Gosling.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

When the images were ready, Mark pressed “start” on FakeApp, and the
training began. His computer screen filled with images of my face and
Mr. Gosling’s face, as the program tried to identify patterns and
similarities.

About eight hours later, after our model had been sufficiently trained,
Mark used FakeApp to finish putting my face on Mr. Gosling’s body. The
video was blurry and bizarre, and Mr. Gosling’s face occasionally
flickered into view. Only the legally blind would mistake the person in
the video for me.

</div>

</div>

<div style="max-width:100%;margin:0 auto">

<div id="100000005776366" class="css-17dprlf" data-slug="roose-deepfake-loop-2" style="max-width:640px">

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

We did better with a clip of Chris Pratt, the scruffy star of “Jurassic
World,” whose face shape is a little more similar to mine. For this
test, Mark used a bigger data set — 1,861 photos of me, 1,023 of him —
and let the model run overnight.

</div>

</div>

<div style="max-width:100%;margin:0 auto">

<div id="100000005776397" class="css-17dprlf" data-slug="roose-deepfake-loop-3" style="max-width:640px">

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

A few days later, Derpfakes, who had also been training a model, sent me
a finished deepfake<span class="css-8l6xbc evw5hdy0"> </span>made using
the footage I had sent and a video of the actor Jake Gyllenhaal. This
one was much more lifelike, a true hybrid that mixed my facial features
with his hair, beard and body.

</div>

</div>

<div style="max-width:100%;margin:0 auto">

<div id="100000005776425" class="css-17dprlf" data-slug="roose-deepfake-loop-4" style="max-width:640px">

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Derpfakes repeated the process with videos of Jimmy Kimmel and Liev
Schreiber, both of which turned out well. As an experienced deepfake
creator, Derpfakes had a more intuitive sense of which source videos
would produce a clean result, and more experience with the subtle
blending and tweaking that takes place at the end of the deepfake
process.

</div>

</div>

<div style="max-width:100%;margin:0 auto">

<div id="100000005776442" class="css-17dprlf" data-slug="roose-deepfake-loop-5" style="max-width:640px">

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

In all, our deepfake experiment took three days and cost $85.96 in
Google Cloud Platform credits. That seemed like a small price to pay for
stardom.

## What the App’s Creator Says

After the experiment, I reached out to the anonymous creator of FakeApp
through an email address on its website. I wanted to know how it felt to
create a cutting-edge A.I. tool, only to have it gleefully co-opted by
ethically challenged pornographers.

A man wrote back, identifying himself as a software developer in
Maryland. Like Derpfakes, the man would not give me his full name, and
instead went by his first initial, N. He said he had created FakeApp as
a creative experiment and was chagrined to see Reddit’s deepfake
community use it for ill.

“I joined the community based around these algorithms when it was very
small (less than 500 people),” he wrote, “and as soon as I saw the
results I knew this was brilliant tech that should be accessible to
anyone who wants to play around with it. I figured I’d take a shot at
putting together an easy-to-use package to accomplish that.”

N. said he didn’t support the use of FakeApp to create nonconsensual
pornography or other abusive content. And he said he agreed with
Reddit’s decision to ban explicit deepfakes. But he defended the
product.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

“I’ve given it a lot of thought,” he said, “and ultimately I’ve decided
I don’t think it’s right to condemn the technology itself — which can of
course be used for many purposes, good and bad.”

FakeApp is somewhat finicky and hard to use, but it’s easy to imagine it
improving quickly. N. said that in the future, FakeApp could be used by
all kinds of people to bring high-budget special effects to their
personal projects.

Deep learning algorithms, he added, were going to be important in the
future, not only as stand-alone apps but as powerful components of many
tech products.

“It’s precisely the things that make them so powerful and useful that
make them so scary,” he said. “There’s really no limit to what you can
apply it to with a little imagination.”

## ‘Next Form of Communication’

On the day of the school shooting last month in Parkland, Fla., a
screenshot of a BuzzFeed News article, “Why We Need to Take Away White
People’s Guns Now More Than Ever,” written by a reporter named Richie
Horowitz, began making the rounds on social media.

The whole thing was fake. No BuzzFeed employee named Richie Horowitz
exists, and [no article with that title was ever
published](https://www.snopes.com/buzzfeed-white-people-guns/) on the
site. But the doctored image pulsed through right-wing outrage channels
and was boosted by activists on Twitter. It wasn’t an A.I.-generated
deepfake, or even a particularly sophisticated Photoshop job, but it did
the trick.

Online misinformation, no matter how sleekly produced, spreads through a
familiar process once it enters our social distribution channels. The
hoax gets 50,000 shares, and the debunking an hour later gets 200. The
carnival barker gets an algorithmic boost on services like Facebook and
YouTube, while the expert screams into the void.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

There’s no reason to believe that deepfake videos will operate any
differently. People will share them when they’re ideologically
convenient and dismiss them when they’re not. The dupes who fall for
satirical stories from The Onion will be fooled by deepfakes, and the
scrupulous people who care about the truth will find ways to detect and
debunk them.

“There’s no choice,” said Hao Li, an assistant professor of computer
science at the University of Southern California. Mr. Li, who is also
the founder of Pinscreen, a company that uses artificial intelligence to
create lifelike 3-D avatars, said the weaponization of A.I. was
inevitable and would require a sudden shift in public awareness.

“I see this as the next form of communication,” he said. “I worry that
people will use it to blackmail others, or do bad things. You have to
educate people that this is possible.”

So, O.K. Here I am, telling you this: An A.I. program powerful enough to
turn Michelle Obama into a pornography star, or transform a schlubby
newspaper columnist into Jake Gyllenhaal, is in our midst. Manipulated
video will soon become far more commonplace.

And there’s probably nothing we can do except try to bat the fakes down
as they happen, pressure social media companies to fight misinformation
aggressively, and trust our eyes a little less every day.

Godspeed, rebels.

</div>

</div>

</div>

<div>

</div>

<div>

</div>

<div>

</div>

<div>

<div id="bottom-wrapper" class="css-1ede5it">

<div id="bottom-slug" class="css-l9onyx">

Advertisement

</div>

[Continue reading the main story](#after-bottom)

<div id="bottom" class="ad bottom-wrapper" style="text-align:center;height:100%;display:block;min-height:90px">

</div>

<div id="after-bottom">

</div>

</div>

</div>

</div>

</div>

## Site Index

<div>

</div>

## Site Information Navigation

  - [© <span>2020</span> <span>The New York Times
    Company</span>](https://help.nytimes.com/hc/en-us/articles/115014792127-Copyright-notice)

<!-- end list -->

  - [NYTCo](https://www.nytco.com/)
  - [Contact
    Us](https://help.nytimes.com/hc/en-us/articles/115015385887-Contact-Us)
  - [Work with us](https://www.nytco.com/careers/)
  - [Advertise](https://nytmediakit.com/)
  - [T Brand Studio](http://www.tbrandstudio.com/)
  - [Your Ad
    Choices](https://www.nytimes.com/privacy/cookie-policy#how-do-i-manage-trackers)
  - [Privacy](https://www.nytimes.com/privacy)
  - [Terms of
    Service](https://help.nytimes.com/hc/en-us/articles/115014893428-Terms-of-service)
  - [Terms of
    Sale](https://help.nytimes.com/hc/en-us/articles/115014893968-Terms-of-sale)
  - [Site Map](https://spiderbites.nytimes.com)
  - [Help](https://help.nytimes.com/hc/en-us)
  - [Subscriptions](https://www.nytimes.com/subscription?campaignId=37WXW)

</div>

</div>

</div>

</div>
