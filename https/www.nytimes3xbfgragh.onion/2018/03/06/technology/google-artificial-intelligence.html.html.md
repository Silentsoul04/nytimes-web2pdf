<div id="app">

<div>

<div>

<div>

<div class="NYTAppHideMasthead css-1q2w90k e1suatyy0">

<div class="section css-ui9rw0 e1suatyy2">

<div class="css-eph4ug er09x8g0">

<div class="css-6n7j50">

</div>

<span class="css-1dv1kvn">Sections</span>

<div class="css-10488qs">

<span class="css-1dv1kvn">SEARCH</span>

</div>

[Skip to content](#site-content)[Skip to site
index](#site-index)

</div>

<div id="masthead-section-label" class="css-1wr3we4 eaxe0e00">

[Technology](https://www.nytimes3xbfgragh.onion/section/technology)

</div>

<div class="css-10698na e1huz5gh0">

</div>

</div>

<div id="masthead-bar-one" class="section hasLinks css-15hmgas e1csuq9d3">

<div class="css-uqyvli e1csuq9d0">

</div>

<div class="css-1uqjmks e1csuq9d1">

</div>

<div class="css-9e9ivx">

[](https://myaccount.nytimes3xbfgragh.onion/auth/login?response_type=cookie&client_id=vi)

</div>

<div class="css-1bvtpon e1csuq9d2">

[Today’s
Paper](https://www.nytimes3xbfgragh.onion/section/todayspaper)

</div>

</div>

</div>

</div>

<div data-aria-hidden="false">

<div id="site-content" data-role="main">

<div>

<div class="css-1aor85t" style="opacity:0.000000001;z-index:-1;visibility:hidden">

<div class="css-1hqnpie">

<div class="css-epjblv">

<span class="css-17xtcya">[Technology](/section/technology)</span><span class="css-x15j1o">|</span><span class="css-fwqvlz">Google
Researchers Are Learning How Machines
Learn</span>

</div>

<div class="css-k008qs">

<div class="css-1iwv8en">

<span class="css-18z7m18"></span>

<div>

</div>

</div>

<span class="css-1n6z4y">https://nyti.ms/2D4lBPx</span>

<div class="css-1705lsu">

<div class="css-4xjgmj">

<div class="css-4skfbu" data-role="toolbar" data-aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count" data-testid="share-tools">

  - 
  - 
  - 
  - 
    
    <div class="css-6n7j50">
    
    </div>

  - 
  - 

</div>

</div>

</div>

</div>

</div>

</div>

<div id="NYT_TOP_BANNER_REGION" class="css-13pd83m">

</div>

<div id="top-wrapper" class="css-1sy8kpn">

<div id="top-slug" class="css-l9onyx">

Advertisement

</div>

[Continue reading the main
story](#after-top)

<div class="ad top-wrapper" style="text-align:center;height:100%;display:block;min-height:250px">

<div id="top" class="place-ad" data-position="top" data-size-key="top">

</div>

</div>

<div id="after-top">

</div>

</div>

<div id="sponsor-wrapper" class="css-1hyfx7x">

<div id="sponsor-slug" class="css-19vbshk">

Supported by

</div>

[Continue reading the main
story](#after-sponsor)

<div id="sponsor" class="ad sponsor-wrapper" style="text-align:center;height:100%;display:block">

</div>

<div id="after-sponsor">

</div>

</div>

<div class="css-1vkm6nb ehdk2mb0">

# Google Researchers Are Learning How Machines Learn

</div>

<div class="css-79elbk" data-testid="photoviewer-wrapper">

<div class="css-z3e15g" data-testid="photoviewer-wrapper-hidden">

</div>

<div class="css-1a48zt4 ehw59r15" data-testid="photoviewer-children">

![<span class="css-16f3y1r e13ogyst0" data-aria-hidden="true">On the
left is an image that was put through a neural network trained to
classify objects in images — for example, to tell whether an image
includes a vase or a lemon. On the right is a visualization of what one
layer in the middle of the network detected at each position of the
image. The neural network seems to be detecting vase-like patterns and
lemon-like
objects.</span><span class="css-cnj6d5 e1z0qqy90" itemprop="copyrightHolder"><span class="css-1ly73wi e1tej78p0">Credit...</span><span><span>The
Building Blocks of
Interpretability</span></span></span>](https://static01.graylady3jvrrxbe.onion/images/2018/03/07/business/07knowbleaiPRINT1/merlin_135059535_d983210f-01f1-4f03-ac7f-79a2662b767e-articleLarge.jpg?quality=75&auto=webp&disable=upscale)

</div>

</div>

<div class="css-xt80pu e12qa4dv0">

<div class="css-18e8msd">

<div class="css-vp77d3 epjyd6m0">

<div class="css-1baulvz">

By [<span class="css-1baulvz last-byline" itemprop="name">Cade
Metz</span>](https://www.nytimes3xbfgragh.onion/by/cade-metz)

</div>

</div>

  - March 6,
    2018

  - 
    
    <div class="css-4xjgmj">
    
    <div class="css-d8bdto" data-role="toolbar" data-aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count" data-testid="share-tools">
    
      - 
      - 
      - 
      - 
        
        <div class="css-6n7j50">
        
        </div>
    
      - 
      - 
    
    </div>
    
    </div>

</div>

</div>

<div class="section meteredContent css-1r7ky0e" name="articleBody" itemprop="articleBody">

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

SAN FRANCISCO — Machines are starting to learn tasks on their own. They
are identifying faces, recognizing spoken words, reading medical scans
and [even carrying on their own
conversations](https://www.nytimes3xbfgragh.onion/interactive/2018/02/21/technology/conversational-bots.html).

</div>

</div>

<div class="css-79elbk" data-testid="photoviewer-wrapper">

<div class="css-z3e15g" data-testid="photoviewer-wrapper-hidden">

</div>

<div class="css-1a48zt4 ehw59r15" data-testid="photoviewer-children">

![<span class="css-16f3y1r e13ogyst0" data-aria-hidden="true">The
original image and three more visualizations after it was put through a
neural network. The first layer is primarily detecting edges and color.
The other layers begin recognizing more complex concepts like flowers,
vases and
lemons.</span><span class="css-cnj6d5 e1z0qqy90" itemprop="copyrightHolder"><span class="css-1ly73wi e1tej78p0">Credit...</span><span>The
Building Blocks of
Interpretability</span></span>](https://static01.graylady3jvrrxbe.onion/images/2018/03/05/business/00knowbleai-2/merlin_135059532_4fef23f6-8615-424f-a784-ce0e2cee7f23-articleLarge.jpg?quality=75&auto=webp&disable=upscale)

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

All this is done through so-called [neural
networks](https://www.wired.com/2015/04/jeff-dean/), which are complex
computer algorithms that learn tasks by analyzing vast amounts of data.
But these neural networks create a problem that scientists are trying to
solve: It is not always easy to tell how the machines arrive at their
conclusions.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

On Tuesday, a team at Google took a small step toward addressing this
issue with the [unveiling of new
research](https://distill.pub/2018/building-blocks)that offers the rough
outlines of technology that shows how the machines are arriving at their
decisions.

</div>

</div>

<div class="css-79elbk" data-testid="photoviewer-wrapper">

<div class="css-z3e15g" data-testid="photoviewer-wrapper-hidden">

</div>

<div class="css-1a48zt4 ehw59r15" data-testid="photoviewer-children">

<div class="css-1xdhyk6 erfvjey0">

<span class="css-1ly73wi e1tej78p0">Image</span>

<div class="css-zjzyr8">

<div data-testid="lazyimage-container" style="height:296.44444444444446px">

</div>

</div>

</div>

<span class="css-16f3y1r e13ogyst0" data-aria-hidden="true">Groups of
neurons automatically learn to work together to represent concepts in an
image. Five groups of neurons seem to correspond to flowers, the lip of
the vase, the body of vase, the background, and lemons. A heat map shows
where each neuron group fired on the
image.</span><span class="css-cnj6d5 e1z0qqy90" itemprop="copyrightHolder"><span class="css-1ly73wi e1tej78p0">Credit...</span><span>The
Building Blocks of Interpretability</span></span>

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

“Even seeing part of how a decision was made can give you a lot of
insight into the possible ways it can fail,” said Christopher Olah, a
Google researcher.

A growing number of A.I. researchers are now developing ways to better
understand neural networks. Jeff Clune, a professor at University of
Wyoming who now works in the A.I. lab at the ride-hailing company Uber,
called this “artificial
neuroscience.”

</div>

</div>

<div class="css-79elbk" data-testid="photoviewer-wrapper">

<div class="css-z3e15g" data-testid="photoviewer-wrapper-hidden">

</div>

<div class="css-1a48zt4 ehw59r15" data-testid="photoviewer-children">

<div class="css-1xdhyk6 erfvjey0">

<span class="css-1ly73wi e1tej78p0">Image</span>

<div class="css-zjzyr8">

<div data-testid="lazyimage-container" style="height:188.17777777777775px">

</div>

</div>

</div>

<span class="css-16f3y1r e13ogyst0" data-aria-hidden="true">Neuron
groups at two different layers of the network and the output classes.
The lines show which neuron groups support or inhibit later groups or
output classes. For example, a \\“lemon\\" classification is strongly
supported by a yellow, lemon-y
group.</span><span class="css-cnj6d5 e1z0qqy90" itemprop="copyrightHolder"><span class="css-1ly73wi e1tej78p0">Credit...</span><span>The
Building Blocks of Interpretability</span></span>

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Understanding how these systems work will become more important as they
make decisions now made by humans, like who gets a job and how a
self-driving car responds to emergencies.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

First proposed in the 1950s, neural networks are meant to mimic the web
of neurons in the brain. But that is a rough analogy. These algorithms
are really series of mathematical operations, and each operation
represents a neuron. Google’s new research aims to show — in a highly
visual way — how these mathematical operations perform discrete tasks,
like recognizing objects in
photos.

</div>

</div>

<div class="css-79elbk" data-testid="photoviewer-wrapper">

<div class="css-z3e15g" data-testid="photoviewer-wrapper-hidden">

</div>

<div class="css-1a48zt4 ehw59r15" data-testid="photoviewer-children">

<div class="css-1xdhyk6 erfvjey0">

<span class="css-1ly73wi e1tej78p0">Image</span>

<div class="css-zjzyr8">

<div data-testid="lazyimage-container" style="height:188.17777777777775px">

</div>

</div>

</div>

<span class="css-16f3y1r e13ogyst0" data-aria-hidden="true">A \\“vase\\"
classification is supported by the groups that represent flowers, the
lip of the vase and the
background.</span><span class="css-cnj6d5 e1z0qqy90" itemprop="copyrightHolder"><span class="css-1ly73wi e1tej78p0">Credit...</span><span>The
Building Blocks of Interpretability</span></span>

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Inside a neural network, each neuron works to identify a particular
characteristic that might show up in a photo, like a line that curves
from right to left at a certain angle or several lines that merge to
form a larger shape. Google wants to provide tools that show what each
neuron is trying to identify, which ones are successful and how their
efforts combine to determine what is actually in the photo — perhaps a
dog or a tuxedo or a bird.

The kind of technology Google is discussing could also help identify why
a neural network is prone to mistakes and, in some cases, explain how it
learned this behavior, Mr. Olah said. Other researchers, including Mr.
Clune, believe they can also help minimize the threat of “adversarial
examples” — where someone can potentially fool neural networks by, say,
doctoring an image.

Researchers acknowledge that this work is still in its infancy. Jason
Yosinski, who also works in Uber’s A.I. lab, which grew out of the
company’s acquisition of a start-up called Geometric Intelligence,
called Google’s technology idea “state of art.” But he warned it may
never be entirely easy to understand the computer mind.

“To a certain extent, as these networks get more complicated, it is
going to be fundamentally difficult to understand why they make
decisions,” he said. “It is kind of like trying to understand why humans
make decisions.”

</div>

</div>

</div>

<div>

</div>

<div>

</div>

<div>

</div>

<div>

<div id="bottom-wrapper" class="css-1ede5it">

<div id="bottom-slug" class="css-l9onyx">

Advertisement

</div>

[Continue reading the main
story](#after-bottom)

<div id="bottom" class="ad bottom-wrapper" style="text-align:center;height:100%;display:block;min-height:90px">

</div>

<div id="after-bottom">

</div>

</div>

</div>

</div>

</div>

## Site Index

<div>

</div>

## Site Information Navigation

  - [© <span>2020</span> <span>The New York Times
    Company</span>](https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014792127-Copyright-notice)

<!-- end list -->

  - [NYTCo](https://www.nytco.com/)
  - [Contact
    Us](https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115015385887-Contact-Us)
  - [Work with us](https://www.nytco.com/careers/)
  - [Advertise](https://nytmediakit.com/)
  - [T Brand Studio](http://www.tbrandstudio.com/)
  - [Your Ad
    Choices](https://www.nytimes3xbfgragh.onion/privacy/cookie-policy#how-do-i-manage-trackers)
  - [Privacy](https://www.nytimes3xbfgragh.onion/privacy)
  - [Terms of
    Service](https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014893428-Terms-of-service)
  - [Terms of
    Sale](https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014893968-Terms-of-sale)
  - [Site
    Map](https://spiderbites.nytimes3xbfgragh.onion)
  - [Help](https://help.nytimes3xbfgragh.onion/hc/en-us)
  - [Subscriptions](https://www.nytimes3xbfgragh.onion/subscription?campaignId=37WXW)

</div>

</div>

</div>

</div>
