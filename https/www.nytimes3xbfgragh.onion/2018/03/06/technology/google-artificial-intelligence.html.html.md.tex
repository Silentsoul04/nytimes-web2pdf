Sections

SEARCH

\protect\hyperlink{site-content}{Skip to
content}\protect\hyperlink{site-index}{Skip to site index}

\href{https://www.nytimes3xbfgragh.onion/section/technology}{Technology}

\href{https://myaccount.nytimes3xbfgragh.onion/auth/login?response_type=cookie\&client_id=vi}{}

\href{https://www.nytimes3xbfgragh.onion/section/todayspaper}{Today's
Paper}

\href{/section/technology}{Technology}\textbar{}Google Researchers Are
Learning How Machines Learn

\url{https://nyti.ms/2D4lBPx}

\begin{itemize}
\item
\item
\item
\item
\item
\item
\end{itemize}

Advertisement

\protect\hyperlink{after-top}{Continue reading the main story}

Supported by

\protect\hyperlink{after-sponsor}{Continue reading the main story}

\hypertarget{google-researchers-are-learning-how-machines-learn}{%
\section{Google Researchers Are Learning How Machines
Learn}\label{google-researchers-are-learning-how-machines-learn}}

\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2018/03/07/business/07knowbleaiPRINT1/merlin_135059535_d983210f-01f1-4f03-ac7f-79a2662b767e-articleLarge.jpg?quality=75\&auto=webp\&disable=upscale}

By \href{https://www.nytimes3xbfgragh.onion/by/cade-metz}{Cade Metz}

\begin{itemize}
\item
  March 6, 2018
\item
  \begin{itemize}
  \item
  \item
  \item
  \item
  \item
  \item
  \end{itemize}
\end{itemize}

SAN FRANCISCO --- Machines are starting to learn tasks on their own.
They are identifying faces, recognizing spoken words, reading medical
scans and
\href{https://www.nytimes3xbfgragh.onion/interactive/2018/02/21/technology/conversational-bots.html}{even
carrying on their own conversations}.

\includegraphics{https://static01.graylady3jvrrxbe.onion/images/2018/03/05/business/00knowbleai-2/merlin_135059532_4fef23f6-8615-424f-a784-ce0e2cee7f23-articleLarge.jpg?quality=75\&auto=webp\&disable=upscale}

All this is done through so-called
\href{https://www.wired.com/2015/04/jeff-dean/}{neural networks}, which
are complex computer algorithms that learn tasks by analyzing vast
amounts of data. But these neural networks create a problem that
scientists are trying to solve: It is not always easy to tell how the
machines arrive at their conclusions.

On Tuesday, a team at Google took a small step toward addressing this
issue with the \href{https://distill.pub/2018/building-blocks}{unveiling
of new research}that offers the rough outlines of technology that shows
how the machines are arriving at their decisions.

Image

Groups of neurons automatically learn to work together to represent
concepts in an image. Five groups of neurons seem to correspond to
flowers, the lip of the vase, the body of vase, the background, and
lemons. A heat map shows where each neuron group fired on the
image.Credit...The Building Blocks of Interpretability

``Even seeing part of how a decision was made can give you a lot of
insight into the possible ways it can fail,'' said Christopher Olah, a
Google researcher.

A growing number of A.I. researchers are now developing ways to better
understand neural networks. Jeff Clune, a professor at University of
Wyoming who now works in the A.I. lab at the ride-hailing company Uber,
called this ``artificial neuroscience.''

Image

Neuron groups at two different layers of the network and the output
classes. The lines show which neuron groups support or inhibit later
groups or output classes. For example, a
\textbackslash{}``lemon\textbackslash{}" classification is strongly
supported by a yellow, lemon-y group.Credit...The Building Blocks of
Interpretability

Understanding how these systems work will become more important as they
make decisions now made by humans, like who gets a job and how a
self-driving car responds to emergencies.

First proposed in the 1950s, neural networks are meant to mimic the web
of neurons in the brain. But that is a rough analogy. These algorithms
are really series of mathematical operations, and each operation
represents a neuron. Google's new research aims to show --- in a highly
visual way --- how these mathematical operations perform discrete tasks,
like recognizing objects in photos.

Image

A \textbackslash{}``vase\textbackslash{}" classification is supported by
the groups that represent flowers, the lip of the vase and the
background.Credit...The Building Blocks of Interpretability

Inside a neural network, each neuron works to identify a particular
characteristic that might show up in a photo, like a line that curves
from right to left at a certain angle or several lines that merge to
form a larger shape. Google wants to provide tools that show what each
neuron is trying to identify, which ones are successful and how their
efforts combine to determine what is actually in the photo --- perhaps a
dog or a tuxedo or a bird.

The kind of technology Google is discussing could also help identify why
a neural network is prone to mistakes and, in some cases, explain how it
learned this behavior, Mr. Olah said. Other researchers, including Mr.
Clune, believe they can also help minimize the threat of ``adversarial
examples'' --- where someone can potentially fool neural networks by,
say, doctoring an image.

Researchers acknowledge that this work is still in its infancy. Jason
Yosinski, who also works in Uber's A.I. lab, which grew out of the
company's acquisition of a start-up called Geometric Intelligence,
called Google's technology idea ``state of art.'' But he warned it may
never be entirely easy to understand the computer mind.

``To a certain extent, as these networks get more complicated, it is
going to be fundamentally difficult to understand why they make
decisions,'' he said. ``It is kind of like trying to understand why
humans make decisions.''

Advertisement

\protect\hyperlink{after-bottom}{Continue reading the main story}

\hypertarget{site-index}{%
\subsection{Site Index}\label{site-index}}

\hypertarget{site-information-navigation}{%
\subsection{Site Information
Navigation}\label{site-information-navigation}}

\begin{itemize}
\tightlist
\item
  \href{https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014792127-Copyright-notice}{Â©~2020~The
  New York Times Company}
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://www.nytco.com/}{NYTCo}
\item
  \href{https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115015385887-Contact-Us}{Contact
  Us}
\item
  \href{https://www.nytco.com/careers/}{Work with us}
\item
  \href{https://nytmediakit.com/}{Advertise}
\item
  \href{http://www.tbrandstudio.com/}{T Brand Studio}
\item
  \href{https://www.nytimes3xbfgragh.onion/privacy/cookie-policy\#how-do-i-manage-trackers}{Your
  Ad Choices}
\item
  \href{https://www.nytimes3xbfgragh.onion/privacy}{Privacy}
\item
  \href{https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014893428-Terms-of-service}{Terms
  of Service}
\item
  \href{https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014893968-Terms-of-sale}{Terms
  of Sale}
\item
  \href{https://spiderbites.nytimes3xbfgragh.onion}{Site Map}
\item
  \href{https://help.nytimes3xbfgragh.onion/hc/en-us}{Help}
\item
  \href{https://www.nytimes3xbfgragh.onion/subscription?campaignId=37WXW}{Subscriptions}
\end{itemize}
