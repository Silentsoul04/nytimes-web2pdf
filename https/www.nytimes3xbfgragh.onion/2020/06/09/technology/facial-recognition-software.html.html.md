<div id="app">

<div>

<div>

<div>

<div class="NYTAppHideMasthead css-1q2w90k e1suatyy0">

<div class="section css-ui9rw0 e1suatyy2">

<div class="css-eph4ug er09x8g0">

<div class="css-6n7j50">

</div>

<span class="css-1dv1kvn">Sections</span>

<div class="css-10488qs">

<span class="css-1dv1kvn">SEARCH</span>

</div>

[Skip to content](#site-content)[Skip to site
index](#site-index)

</div>

<div id="masthead-section-label" class="css-1wr3we4 eaxe0e00">

[Technology](https://www.nytimes3xbfgragh.onion/section/technology)

</div>

<div class="css-10698na e1huz5gh0">

</div>

</div>

<div id="masthead-bar-one" class="section hasLinks css-15hmgas e1csuq9d3">

<div class="css-uqyvli e1csuq9d0">

</div>

<div class="css-1uqjmks e1csuq9d1">

</div>

<div class="css-9e9ivx">

[](https://myaccount.nytimes3xbfgragh.onion/auth/login?response_type=cookie&client_id=vi)

</div>

<div class="css-1bvtpon e1csuq9d2">

[Today’s
Paper](https://www.nytimes3xbfgragh.onion/section/todayspaper)

</div>

</div>

</div>

</div>

<div data-aria-hidden="false">

<div id="site-content" data-role="main">

<div>

<div class="css-1aor85t" style="opacity:0.000000001;z-index:-1;visibility:hidden">

<div class="css-1hqnpie">

<div class="css-epjblv">

<span class="css-17xtcya">[Technology](/section/technology)</span><span class="css-x15j1o">|</span><span class="css-fwqvlz">A
Case for Banning Facial
Recognition</span>

</div>

<div class="css-k008qs">

<div class="css-1iwv8en">

<span class="css-18z7m18"></span>

<div>

</div>

</div>

<span class="css-1n6z4y">https://nyti.ms/2ARmEqs</span>

<div class="css-1705lsu">

<div class="css-4xjgmj">

<div class="css-4skfbu" data-role="toolbar" data-aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count" data-testid="share-tools">

  - 
  - 
  - 
  - 
    
    <div class="css-6n7j50">
    
    </div>

  - 

</div>

</div>

</div>

</div>

</div>

</div>

<div id="NYT_TOP_BANNER_REGION" class="css-13pd83m">

</div>

<div id="top-wrapper" class="css-1sy8kpn">

<div id="top-slug" class="css-l9onyx">

Advertisement

</div>

[Continue reading the main
story](#after-top)

<div class="ad top-wrapper" style="text-align:center;height:100%;display:block;min-height:250px">

<div id="top" class="place-ad" data-position="top" data-size-key="top">

</div>

</div>

<div id="after-top">

</div>

</div>

<div>

<div id="sponsor-wrapper" class="css-1hyfx7x">

<div id="sponsor-slug" class="css-19vbshk">

Supported by

</div>

[Continue reading the main
story](#after-sponsor)

<div id="sponsor" class="ad sponsor-wrapper" style="text-align:center;height:100%;display:block">

</div>

<div id="after-sponsor">

</div>

</div>

<div class="css-186x18t">

on tech

</div>

<div class="css-1vkm6nb ehdk2mb0">

# A Case for Banning Facial Recognition

</div>

A Google research scientist explains why she thinks the police shouldn’t
use facial recognition
software.

![<span class="css-cch8ym"><span class="css-1dv1kvn">Credit</span><span class="css-cnj6d5 e1z0qqy90" itemprop="copyrightHolder"><span class="css-1ly73wi e1tej78p0">Credit...</span><span>By
Ziv
Schneider</span></span></span>](https://static01.graylady3jvrrxbe.onion/images/2020/06/09/business/09ontech-videostill/09ontech-videostill-threeByTwoMediumAt2X.png)

<div class="css-18e8msd">

<div class="css-vp77d3 epjyd6m0">

<div class="css-hus3qt ey68jwv0" data-aria-hidden="true">

[![Shira
Ovide](https://static01.graylady3jvrrxbe.onion/images/2020/03/18/reader-center/author-shira-ovide/author-shira-ovide-thumbLarge-v2.png
"Shira Ovide")](https://www.nytimes3xbfgragh.onion/by/shira-ovide)

</div>

<div class="css-1baulvz">

By [<span class="css-1baulvz last-byline" itemprop="name">Shira
Ovide</span>](https://www.nytimes3xbfgragh.onion/by/shira-ovide)

</div>

</div>

  - 
    
    <div class="css-ld3wwf e16638kd2">
    
    June 9,
    2020
    
    </div>

  - 
    
    <div class="css-4xjgmj">
    
    <div class="css-d8bdto" data-role="toolbar" data-aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count" data-testid="share-tools">
    
      - 
      - 
      - 
      - 
        
        <div class="css-6n7j50">
        
        </div>
    
      - 
    
    </div>
    
    </div>

</div>

</div>

<div class="section meteredContent css-1r7ky0e" name="articleBody" itemprop="articleBody">

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

*This article is part of the On Tech newsletter. You can* [*sign up
here*](https://www.nytimes3xbfgragh.onion/newsletters/signup/OT) *to
receive it weekdays.*

Facial recognition software might be the world’s most
[divisive](https://www.nytimes3xbfgragh.onion/2019/05/15/business/facial-recognition-software-controversy.html)
technology.

Law enforcement agencies and some companies use it to
[identify](https://www.nytimes3xbfgragh.onion/2019/06/09/opinion/facial-recognition-police-new-york-city.html)
suspects and victims by matching photos and video with databases like
driver’s license records. But civil liberties groups say facial
recognition [contributes to privacy
erosion](https://www.nytimes3xbfgragh.onion/2020/01/18/technology/clearview-privacy-facial-recognition.html),
[reinforces
bias](https://www.nytimes3xbfgragh.onion/2019/12/19/technology/facial-recognition-bias.html)
against black people and is prone to
[misuse](https://www.washingtonpost.com/technology/2019/04/30/amazons-facial-recognition-technology-is-supercharging-local-police/).

[San
Francisco](https://www.nytimes3xbfgragh.onion/2019/05/14/us/facial-recognition-ban-san-francisco.html)
and a major [provider of police body
cameras](https://www.nytimes3xbfgragh.onion/2019/06/27/opinion/police-cam-facial-recognition.html)
have barred its use by law enforcement, and IBM on Monday [backed away
from its
work](https://www.axios.com/ibm-is-exiting-the-face-recognition-business-62e79f09-34a2-4f1d-a541-caba112415c6.html)
in this area. Some proposals to restructure police departments call for
tighter
[restrictions](https://www.protocol.com/police-facial-recognition-legislation)
on their use of facial recognition.

Timnit Gebru, a leader of Google’s ethical artificial intelligence team,
explained why she believes that facial recognition is too dangerous to
be used right now for law enforcement purposes. These are edited
excerpts from our virtual discussion at the [Women’s Forum for the
Economy & Society](http://www.womens-forum.com/) on Monday.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

**Ovide: What are your concerns about facial recognition?**

**Gebru:** I collaborated with [Joy
Buolamwini](https://www.ajlunited.org/about) at the M.I.T. Media Lab,
who led an [analysis](http://gendershades.org/) that found [very high
disparities in error
rates](https://www.nytimes3xbfgragh.onion/2018/06/21/opinion/facial-analysis-technology-bias.html)
\[in facial identification systems\], especially between lighter-skinned
men and darker-skinned women. In melanoma screenings, imagine that
there’s a detection technology that doesn’t work for people with
darker skin.

I also realized that even perfect facial recognition can be misused. I’m
a black woman living in the U.S. who has dealt with serious consequences
of racism. Facial recognition is being used against the black community.
Baltimore police during the [Freddie Gray
protests](https://www.nytimes3xbfgragh.onion/2016/04/13/us/baltimore-freddie-gray.html)
used [facial recognition to identify
protesters](https://www.baltimoresun.com/news/crime/bs-md-facial-recognition-20161017-story.html)
by linking images to social media profiles.

**But a police officer or eyewitness could also look at surveillance
footage and mug shots and misidentify someone as Jim Smith. Is software
more accurate or less biased than humans?**

That depends. Our analysis showed that for many, facial recognition was
way less accurate than humans.

The other problem is something called automation bias. If your intuition
tells you that an image doesn’t look like Smith, but the computer model
tells you that it is him with 99 percent accuracy, you’re more likely to
believe that model.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

There’s also an imbalance of power. Facial recognition can be completely
accurate, but it can still be used in a way that is detrimental to
certain groups of people.

The combination of overreliance on technology, misuse and lack of
transparency — we don’t know how widespread the use of this software is
— is dangerous.

**A maker of police body cameras recently discussed using artificial
intelligence to** **[analyze video
footage](https://www.bloomberg.com/news/newsletters/2020-06-05/should-police-officers-wear-body-cameras)**
**and possibly flag law-enforcement incidents for review. What’s your
take on using technology in that way?**

My gut reaction is that a lot of people in technology have the urge to
jump on a tech solution without listening to people who have been
working with community leaders, the police and others proposing
solutions to reform the police.

**Do you see a way to use facial recognition for law enforcement and
security responsibly?**

It should be banned at the moment. I don’t know about the future.

*You can watch our entire conversation about helpful uses of A.I. and
its downsides*
[*here*](https://www.youtube.com/watch?v=uYlR3OIUQx4&feature=youtu.be)*.*

-----

### Tip of the Week

## Stopping trackers in their tracks

[*Brian X. Chen*](https://www.nytimes3xbfgragh.onion/by/brian-x-chen)*,
a consumer technology writer at the The New York Times, writes in to
explain ways that emails can identify when and where you click, and how
to dial back the tracking.*

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Google’s Gmail is so popular in large part because its artificial
intelligence is effective at filtering out spam. But it does little to
combat another nuisance: email tracking.

The trackers come in many forms, like an invisible piece of software
inserted into an email or a hyperlink embedded inside text. They are
frequently used to detect when someone opens an email and even a
person’s location when the message is opened.

When used legitimately, email trackers help businesses determine what
types of marketing messages to send to you, and how frequently to
communicate with you. This emailed newsletter has some trackers as well
to help us gain insight into the topics you like to read about, among
other metrics.

But from a privacy perspective, [email
tracking](https://www.getrevue.co/profile/themarkup/issues/hello-from-the-markup-199187)
may feel unfair. You didn’t opt in to being tracked, and there’s no
simple way to opt out.

Fortunately, many email trackers can be thwarted by disabling images
from automatically loading in Gmail messages. Here’s how to do that:

  - Inside Gmail.com, look in the upper right corner for the icon of a
    gear, click on it, and choose the “Settings” option.

  - In the settings window, scroll down to “Images.” Select “Ask before
    displaying external images.”

With this setting enabled, you can prevent tracking software from
loading automatically. If you choose, you can agree to load the images.
This won’t stop all email tracking, but it’s better than nothing.

**Bonus tech tip\!** Some readers asked for more help setting up
notifications that can [alert you to fraudulent credit card
charges](https://www.nytimes3xbfgragh.onion/2020/06/08/technology/how-to-reduce-credit-card-fraud.html).
Signing up for these is not easy because, let’s face it, financial
websites are not the simplest to use.

On the apps and websites for the credit cards I have, I found these
alerts in menus labeled “Profile and Settings” or “Help & Support.” Look
for “Alerts” or dig into the privacy and security options. Sign up for
an email or app notification each time your card is used to make a
purchase online and over the phone.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Most of the time, those purchases are from you. But you want to know
right away in the (hopefully) rare times when they’re not.

-----

## Before we go …

  - **Behind the pro-China Twitter campaign:** An analysis by my New
    York Times colleagues found a new and decidedly pro-China presence
    on Twitter, made up of a network of accounts exhibiting [seemingly
    coordinated
    behavior](https://www.nytimes3xbfgragh.onion/2020/06/08/technology/china-twitter-disinformation.html).
    The findings add to evidence suggesting that Twitter is being
    manipulated to amplify the Chinese government’s messaging about the
    coronavirus and other topics.

  - **Restaurants really aren’t fans of those apps:** [Nathaniel
    Popper](https://www.nytimes3xbfgragh.onion/by/nathaniel-popper), a
    Times tech reporter, explains why [restaurants are increasingly
    unhappy
    about](https://www.nytimes3xbfgragh.onion/2020/06/09/technology/delivery-apps-restaurants-fees-virus.html)the
    high fees and other aspects of food delivery services like Grubhub
    and Postmates. (I’ll have a conversation with Nathaniel about this
    in Wednesday’s newsletter.)

  - **The downsides of every gathering of humans:** The neighborhood
    social network Nextdoor has been both a place for people to help one
    another during the pandemic, and a way for neighbors to lash out at
    one another over perceived slights or fan fears about crime. The
    Verge writes about the challenges faced by the volunteers on
    Nextdoor who are [moderating discussions about
    race](https://www.theverge.com/21283993/nextdoor-app-racism-community-moderation-guidance-protests)
    and the recent protests.

**Hugs to this**

[NPR’s Pop Culture Happy
Hour](https://www.npr.org/podcasts/510282/pop-culture-happy-hour)
recently introduced to me the [whimsical mini children’s
stories](https://twitter.com/AnneLouiseAvery/status/1267931929703317505?s=09)
that the writer Anne Louise Avery [composes on
Twitter](https://twitter.com/AnneLouiseAvery/status/1254385383510589441).

-----

*We want to hear from you. Tell us what you think of this newsletter and
what else you’d like us to explore. You can reach us at*
[*ontech@NYTimes.com.*](mailto:ontech@NYTimes.com?subject=On%20Tech%20Feedback)

*Get this newsletter in your inbox every weekday;*[*please sign up
here*](https://www.nytimes3xbfgragh.onion/newsletters/signup/OT)*.*

</div>

</div>

</div>

<div>

</div>

<div>

</div>

<div>

</div>

<div>

<div id="bottom-wrapper" class="css-1ede5it">

<div id="bottom-slug" class="css-l9onyx">

Advertisement

</div>

[Continue reading the main
story](#after-bottom)

<div id="bottom" class="ad bottom-wrapper" style="text-align:center;height:100%;display:block;min-height:90px">

</div>

<div id="after-bottom">

</div>

</div>

</div>

</div>

</div>

## Site Index

<div>

</div>

## Site Information Navigation

  - [© <span>2020</span> <span>The New York Times
    Company</span>](https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014792127-Copyright-notice)

<!-- end list -->

  - [NYTCo](https://www.nytco.com/)
  - [Contact
    Us](https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115015385887-Contact-Us)
  - [Work with us](https://www.nytco.com/careers/)
  - [Advertise](https://nytmediakit.com/)
  - [T Brand Studio](http://www.tbrandstudio.com/)
  - [Your Ad
    Choices](https://www.nytimes3xbfgragh.onion/privacy/cookie-policy#how-do-i-manage-trackers)
  - [Privacy](https://www.nytimes3xbfgragh.onion/privacy)
  - [Terms of
    Service](https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014893428-Terms-of-service)
  - [Terms of
    Sale](https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014893968-Terms-of-sale)
  - [Site
    Map](https://spiderbites.nytimes3xbfgragh.onion)
  - [Help](https://help.nytimes3xbfgragh.onion/hc/en-us)
  - [Subscriptions](https://www.nytimes3xbfgragh.onion/subscription?campaignId=37WXW)

</div>

</div>

</div>

</div>
