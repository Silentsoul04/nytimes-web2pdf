<div id="app">

<div>

<div>

<div>

<div class="NYTAppHideMasthead css-1q2w90k e1suatyy0">

<div class="section css-ui9rw0 e1suatyy2">

<div class="css-eph4ug er09x8g0">

<div class="css-6n7j50">

</div>

<span class="css-1dv1kvn">Sections</span>

<div class="css-10488qs">

<span class="css-1dv1kvn">SEARCH</span>

</div>

[Skip to content](#site-content)[Skip to site
index](#site-index)

</div>

<div id="masthead-section-label" class="css-1wr3we4 eaxe0e00">

[Technology](https://www.nytimes3xbfgragh.onion/section/technology)

</div>

<div class="css-10698na e1huz5gh0">

</div>

</div>

<div id="masthead-bar-one" class="section hasLinks css-15hmgas e1csuq9d3">

<div class="css-uqyvli e1csuq9d0">

</div>

<div class="css-1uqjmks e1csuq9d1">

</div>

<div class="css-9e9ivx">

[](https://myaccount.nytimes3xbfgragh.onion/auth/login?response_type=cookie&client_id=vi)

</div>

<div class="css-1bvtpon e1csuq9d2">

[Today’s
Paper](https://www.nytimes3xbfgragh.onion/section/todayspaper)

</div>

</div>

</div>

</div>

<div data-aria-hidden="false">

<div id="site-content" data-role="main">

<div>

<div class="css-1aor85t" style="opacity:0.000000001;z-index:-1;visibility:hidden">

<div class="css-1hqnpie">

<div class="css-epjblv">

<span class="css-17xtcya">[Technology](/section/technology)</span><span class="css-x15j1o">|</span><span class="css-fwqvlz">This
Tool Could Protect Your Photos From Facial
Recognition</span>

</div>

<div class="css-k008qs">

<div class="css-1iwv8en">

<span class="css-18z7m18"></span>

<div>

</div>

</div>

<span class="css-1n6z4y">https://nyti.ms/31g6Gjh</span>

<div class="css-1705lsu">

<div class="css-4xjgmj">

<div class="css-4skfbu" data-role="toolbar" data-aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count" data-testid="share-tools">

  - 
  - 
  - 
  - 
    
    <div class="css-6n7j50">
    
    </div>

  - 
  - 

</div>

</div>

</div>

</div>

</div>

</div>

<div id="NYT_TOP_BANNER_REGION" class="css-13pd83m">

</div>

<div id="top-wrapper" class="css-1sy8kpn">

<div id="top-slug" class="css-l9onyx">

Advertisement

</div>

[Continue reading the main
story](#after-top)

<div class="ad top-wrapper" style="text-align:center;height:100%;display:block;min-height:250px">

<div id="top" class="place-ad" data-position="top" data-size-key="top">

</div>

</div>

<div id="after-top">

</div>

</div>

<div>

<div id="sponsor-wrapper" class="css-1hyfx7x">

<div id="sponsor-slug" class="css-19vbshk">

Supported by

</div>

[Continue reading the main
story](#after-sponsor)

<div id="sponsor" class="ad sponsor-wrapper" style="text-align:center;height:100%;display:block">

</div>

<div id="after-sponsor">

</div>

</div>

<div class="css-186x18t">

</div>

<div class="css-1vkm6nb ehdk2mb0">

# This Tool Could Protect Your Photos From Facial Recognition

</div>

Researchers at the University of Chicago want you to be able to post
selfies without worrying that the next Clearview AI will use them to
identify you.

<div class="css-79elbk" data-testid="photoviewer-wrapper">

<div class="css-z3e15g" data-testid="photoviewer-wrapper-hidden">

</div>

<div class="css-1a48zt4 ehw59r15" data-testid="photoviewer-children">

![<span class="css-16f3y1r e13ogyst0" data-aria-hidden="true">Before and
after photographs of, from left, Jessica Simpson, Gwyneth Paltrow and
Patrick Dempsey that were cloaked by the Fawkes
team.</span><span class="css-cnj6d5 e1z0qqy90" itemprop="copyrightHolder"><span class="css-1ly73wi e1tej78p0">Credit...</span><span><span>SAND
Lab, University of
Chicago</span></span></span>](https://static01.graylady3jvrrxbe.onion/images/2020/07/31/business/31fawkes4/merlin_175009401_d9273e2a-5fbf-4229-8e98-44a2cce6b72b-articleLarge.jpg?quality=75&auto=webp&disable=upscale)

</div>

</div>

<div class="css-18e8msd">

<div class="css-vp77d3 epjyd6m0">

<div class="css-hus3qt ey68jwv0" data-aria-hidden="true">

[![Kashmir
Hill](https://static01.graylady3jvrrxbe.onion/images/2020/07/24/business/author-hill-kashmir/author-hill-kashmir-thumbLarge-v2.png
"Kashmir Hill")](https://www.nytimes3xbfgragh.onion/by/kashmir-hill)

</div>

<div class="css-1baulvz">

By [<span class="css-1baulvz last-byline" itemprop="name">Kashmir
Hill</span>](https://www.nytimes3xbfgragh.onion/by/kashmir-hill)

</div>

</div>

  - 
    
    <div class="css-ld3wwf e16638kd2">
    
    Published Aug. 3, 2020Updated Aug. 4, 2020,
    <span class="css-epvm6">12:19 a.m.
    ET</span>
    
    </div>

  - 
    
    <div class="css-4xjgmj">
    
    <div class="css-pvvomx" data-role="toolbar" data-aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count" data-testid="share-tools">
    
      - 
      - 
      - 
      - 
        
        <div class="css-6n7j50">
        
        </div>
    
      - 
      - 
    
    </div>
    
    </div>

</div>

</div>

<div class="section meteredContent css-1r7ky0e" name="articleBody" itemprop="articleBody">

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

In recent years, companies have been
[prowling](https://onezero.medium.com/this-simple-facial-recognition-search-engine-can-track-you-down-across-the-internet-518c7129e454)
the web for public photos associated with people’s names that they can
use to build enormous databases of faces and [improve their facial
recognition
systems](https://www.nytimes3xbfgragh.onion/interactive/2019/10/11/technology/flickr-facial-recognition.html),
adding to a growing sense that personal privacy is being lost, bit by
digital bit.

A start-up called Clearview AI, for example, scraped [billions of online
photos](https://www.nytimes3xbfgragh.onion/2020/01/18/technology/clearview-privacy-facial-recognition.html)
to build a tool for the police that could lead them from a face to a
Facebook account, revealing a person’s identity.

Now researchers are trying to foil those systems. A team of computer
engineers at the University of Chicago has developed a tool that
disguises photos with pixel-level changes that confuse facial
recognition systems.

Named [Fawkes](http://sandlab.cs.uchicago.edu/fawkes/) in honor of the
[Guy Fawkes
mask](https://www.nytimes3xbfgragh.onion/2019/11/05/opinion/guy-fawkes-day-v-for-vendetta.html)
favored by protesters worldwide, the software was made available to
developers on the researchers’ website last month. After being
discovered by [Hacker
News](https://news.ycombinator.com/item?id=23917337), it has been
downloaded more than 50,000 times. The researchers are working on a free
app version for noncoders, which they hope to make available soon.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

The software is not intended to be just a one-off tool for
privacy-loving individuals. If deployed across millions of images, it
would be a broadside against facial recognition systems, poisoning the
accuracy of the so-called data sets they gather from the web.

“Our goal is to make Clearview go away,” said Ben Zhao, a professor of
computer science at the University of Chicago.

Fawkes converts an image — or “cloaks” it, in the researchers’ parlance
— by subtly altering some of the features that facial recognition
systems depend on when they construct a person’s face print. In a
[research paper](https://arxiv.org/pdf/2002.08327.pdf), reported earlier
by
[OneZero](https://onezero.medium.com/this-filter-makes-your-photos-invisible-to-facial-recognition-a26929b5ccf),
the team describes “cloaking” photos of the actress Gwyneth Paltrow
using the actor Patrick Dempsey’s face, so that a system learning what
Ms. Paltrow looks like based on those photos would start associating her
with some of the features of Mr. Dempsey’s face.

The changes, usually subtle and not perceptible to the naked eye, would
prevent the system from recognizing Ms. Paltrow when presented with a
real, uncloaked photo of her. In testing, the researchers were able to
fool facial recognition systems from Amazon, Microsoft and the Chinese
tech company Megvii.

</div>

</div>

<div class="css-79elbk" data-testid="photoviewer-wrapper">

<div class="css-z3e15g" data-testid="photoviewer-wrapper-hidden">

</div>

<div class="css-1a48zt4 ehw59r15" data-testid="photoviewer-children">

![<span class="css-16f3y1r e13ogyst0" data-aria-hidden="true">Fawkes
used Mr. Dempsey’s face to cloak photos of Ms. Paltrow, so that a system
using those images would start associating her with some of the features
of Mr. Dempsey’s
face.</span><span class="css-cnj6d5 e1z0qqy90" itemprop="copyrightHolder"><span class="css-1ly73wi e1tej78p0">Credit...</span><span>SAND
Lab, University of
Chicago</span></span>](https://static01.graylady3jvrrxbe.onion/images/2020/07/27/business/00fawkes3/merlin_175009404_3d4454a7-1d31-400b-ba2e-15e2b2b25d94-articleLarge.jpg?quality=75&auto=webp&disable=upscale)

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

To test the tool, I asked the team to cloak some images of my family and
me. I then uploaded the originals and the cloaked images to Facebook to
see if they fooled the social network’s [facial recognition
system](https://www.facebookcorewwwi.onion/help/122175507864081). It
worked: Facebook tagged me in the original photo but did not recognize
me in the cloaked version.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

However, the changes to the photos were noticeable to the naked eye. In
the altered images, I looked ghoulish, my 3-year-old daughter sprouted
what looked like facial hair, and my husband appeared to have a black
eye.

The researchers had a few explanations for this. One is that the
software is designed to match you with the face template of someone who
looks as much unlike you as possible, pulling from [a
database](http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/) of celebrity
faces. That usually ends up being a person of the opposite sex, which
leads to obvious problems.

“Women get mustaches, and guys get extra eyelashes or eye shadow,” Mr.
Zhao said. He is enthusiastic about what he calls “privacy armor” and
previously helped design [a bracelet that stops smart
speakers](https://www.nytimes3xbfgragh.onion/2020/02/14/technology/alexa-jamming-bracelet-privacy-armor.html)
from overhearing
conversations.

</div>

</div>

<div class="css-79elbk" data-testid="photoviewer-wrapper">

<div class="css-z3e15g" data-testid="photoviewer-wrapper-hidden">

</div>

<div class="css-1a48zt4 ehw59r15" data-testid="photoviewer-children">

<div class="css-1xdhyk6 erfvjey0">

<span class="css-1ly73wi e1tej78p0">Image</span>

<div class="css-zjzyr8">

<div data-testid="lazyimage-container" style="height:196.55555555555557px">

</div>

</div>

</div>

<span class="css-16f3y1r e13ogyst0" data-aria-hidden="true">Left, our
reporter’s original images and, right, the “cloaked” versions.</span>

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

The team says it plans to tweak the software so that it will no longer
subtly change the sex of users.

The other issue is that my experiment wasn’t what the tool was designed
to do, so Shawn Shan, a Ph.D. student at the University of Chicago who
is one of the creators of the Fawkes software, made the changes to my
photos as extreme as possible to ensure that it worked. Fawkes isn’t
intended to keep a facial recognition system like Facebook’s from
recognizing someone in a single photo. It’s trying to more broadly
corrupt facial recognition systems, performing an algorithmic attack
called data poisoning.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

The researchers said that, ideally, people would start cloaking all the
images they uploaded. That would mean a company like Clearview that
scrapes those photos wouldn’t be able to create a functioning database,
because an unidentified photo of you from the real world wouldn’t match
the template of you that Clearview would have built over time from your
online photos.

But Clearview’s chief executive, Hoan Ton-That, ran a version of my
Facebook experiment on the Clearview app and said the technology did not
interfere with his system. In fact, he said, his company could use
images cloaked by Fawkes to improve its ability to make sense of altered
images.

“There are billions of unmodified photos on the internet, all on
different domain names,” Mr. Ton-That said. “In practice, it’s almost
certainly too late to perfect a technology like Fawkes and deploy it at
scale.”

</div>

</div>

<div class="css-79elbk" data-testid="photoviewer-wrapper">

<div class="css-z3e15g" data-testid="photoviewer-wrapper-hidden">

</div>

<div class="css-1a48zt4 ehw59r15" data-testid="photoviewer-children">

<div class="css-1xdhyk6 erfvjey0">

<span class="css-1ly73wi e1tej78p0">Image</span>

<div class="css-zjzyr8">

<div data-testid="lazyimage-container" style="height:257.77777777777777px">

</div>

</div>

</div>

<span class="css-16f3y1r e13ogyst0" data-aria-hidden="true">Hoan
Ton-That, the chief executive of Clearview AI, using the Clearview smart
phone
application.</span><span class="css-cnj6d5 e1z0qqy90" itemprop="copyrightHolder"><span class="css-1ly73wi e1tej78p0">Credit...</span><span>Amr
Alfiky for The New York Times</span></span>

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Other experts were also skeptical that Fawkes would work. Joseph Atick,
a facial recognition pioneer [who has come to
regret](https://www.nytimes3xbfgragh.onion/2014/05/18/technology/never-forgetting-a-face.html)
the surveillance society he helped to create, said the volume of images
of ourselves that we had already made available would be too hard to
overcome.

“The cat is out of the bag. We’re out there,” Dr. Atick said. “While I
encourage this type of research, I’m highly skeptical this is a solution
to solve the problem that we’re faced with.”

Dr. Atick thinks that only lawmakers can ensure that people have a right
to facial anonymity. No such federal law is on the horizon, though
Democratic senators did recently propose a [ban on government use of
facial
recognition](https://www.markey.senate.gov/news/press-releases/senators-markey-and-merkley-and-reps-jayapal-pressley-to-introduce-legislation-to-ban-government-use-of-facial-recognition-other-biometric-technology).

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

“I personally think that no matter which approach you use, you lose,”
said Emily Wenger, a Ph.D. student who helped create Fawkes. “You can
have these technological solutions, but it’s a cat-and-mouse game. And
you can have a law, but there will always be illegal actors.”

Ms. Wenger thinks “a two-prong approach” is needed, where individuals
have technological tools and a privacy law to protect themselves.

Elizabeth Joh, a law professor at the University of California, Davis,
has written about tools like Fawkes as “[privacy
protests](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2285095),”
where individuals want to thwart surveillance but not for criminal
reasons. She has repeatedly seen what she called a “tired rubric" of
surveillance, then countersurveillance and then
anti-countersurveillance, as new monitoring technologies are introduced.

“People are feeling a sense of privacy exhaustion,” Ms. Joh said. “There
are too many ways that our conventional sense of privacy is being
exploited in real life and online.”

For Fawkes to have an immediate effect, we would need all the photos of
ourselves that we had already posted to be cloaked overnight. That could
happen if a huge platform that maintains an enormous number of online
images decided to roll out Fawkes systemwide.

A platform like Facebook’s adopting Fawkes would prevent a future
Clearview from scraping its users’ images to identify them. “They could
say, ‘Give us your real photos, we’ll cloak them, and then we’ll share
them with the world so you’ll be protected,’” Mr. Zhao said.

Jay Nancarrow, a Facebook spokesman, did not rule out that possibility
when asked for comment. “As part of our efforts to protect people’s
privacy, we have a dedicated team exploring this type of technology and
other methods of preventing photo misuse,” Mr. Nancarrow said.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

“I’m actually interning on that exact team at Facebook right now,” said
the Fawkes co-creator Mr. Shan.

</div>

</div>

<div>

</div>

</div>

<div>

</div>

<div>

</div>

<div>

</div>

<div>

<div id="bottom-wrapper" class="css-1ede5it">

<div id="bottom-slug" class="css-l9onyx">

Advertisement

</div>

[Continue reading the main
story](#after-bottom)

<div id="bottom" class="ad bottom-wrapper" style="text-align:center;height:100%;display:block;min-height:90px">

</div>

<div id="after-bottom">

</div>

</div>

</div>

</div>

</div>

## Site Index

<div>

</div>

## Site Information Navigation

  - [© <span>2020</span> <span>The New York Times
    Company</span>](https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014792127-Copyright-notice)

<!-- end list -->

  - [NYTCo](https://www.nytco.com/)
  - [Contact
    Us](https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115015385887-Contact-Us)
  - [Work with us](https://www.nytco.com/careers/)
  - [Advertise](https://nytmediakit.com/)
  - [T Brand Studio](http://www.tbrandstudio.com/)
  - [Your Ad
    Choices](https://www.nytimes3xbfgragh.onion/privacy/cookie-policy#how-do-i-manage-trackers)
  - [Privacy](https://www.nytimes3xbfgragh.onion/privacy)
  - [Terms of
    Service](https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014893428-Terms-of-service)
  - [Terms of
    Sale](https://help.nytimes3xbfgragh.onion/hc/en-us/articles/115014893968-Terms-of-sale)
  - [Site
    Map](https://spiderbites.nytimes3xbfgragh.onion)
  - [Help](https://help.nytimes3xbfgragh.onion/hc/en-us)
  - [Subscriptions](https://www.nytimes3xbfgragh.onion/subscription?campaignId=37WXW)

</div>

</div>

</div>

</div>
