<div id="app">

<div>

<div>

<div>

<div class="NYTAppHideMasthead css-1q2w90k e1suatyy0">

<div class="section css-ui9rw0 e1suatyy2">

<div class="css-eph4ug er09x8g0">

<div class="css-6n7j50">

</div>

<span class="css-1dv1kvn">Sections</span>

<div class="css-10488qs">

<span class="css-1dv1kvn">SEARCH</span>

</div>

[Skip to content](#site-content)[Skip to site
index](#site-index)

</div>

<div id="masthead-section-label" class="css-1wr3we4 eaxe0e00">

[Technology](https://www.nytimes.com/section/technology)

</div>

<div class="css-10698na e1huz5gh0">

</div>

</div>

<div id="masthead-bar-one" class="section hasLinks css-15hmgas e1csuq9d3">

<div class="css-uqyvli e1csuq9d0">

</div>

<div class="css-1uqjmks e1csuq9d1">

</div>

<div class="css-9e9ivx">

[](https://myaccount.nytimes.com/auth/login?response_type=cookie&client_id=vi)

</div>

<div class="css-1bvtpon e1csuq9d2">

[Today’s
Paper](https://www.nytimes.com/section/todayspaper)

</div>

</div>

</div>

</div>

<div data-aria-hidden="false">

<div id="site-content" data-role="main">

<div>

<div class="css-1aor85t" style="opacity:0.000000001;z-index:-1;visibility:hidden">

<div class="css-1hqnpie">

<div class="css-epjblv">

<span class="css-17xtcya">[Technology](/section/technology)</span><span class="css-x15j1o">|</span><span class="css-fwqvlz">Many
Facial-Recognition Systems Are Biased, Says U.S.
Study</span>

</div>

<div class="css-k008qs">

<div class="css-1iwv8en">

<span class="css-18z7m18"></span>

<div>

</div>

</div>

<span class="css-1n6z4y">https://nyti.ms/36SVOZE</span>

<div class="css-1705lsu">

<div class="css-4xjgmj">

<div class="css-4skfbu" data-role="toolbar" data-aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count" data-testid="share-tools">

  - 
  - 
  - 
  - 
    
    <div class="css-6n7j50">
    
    </div>

  - 

</div>

</div>

</div>

</div>

</div>

</div>

<div id="NYT_TOP_BANNER_REGION" class="css-13pd83m">

</div>

<div id="top-wrapper" class="css-1sy8kpn">

<div id="top-slug" class="css-l9onyx">

Advertisement

</div>

[Continue reading the main
story](#after-top)

<div class="ad top-wrapper" style="text-align:center;height:100%;display:block;min-height:250px">

<div id="top" class="place-ad" data-position="top" data-size-key="top">

</div>

</div>

<div id="after-top">

</div>

</div>

<div>

<div id="sponsor-wrapper" class="css-1hyfx7x">

<div id="sponsor-slug" class="css-19vbshk">

Supported by

</div>

[Continue reading the main
story](#after-sponsor)

<div id="sponsor" class="ad sponsor-wrapper" style="text-align:center;height:100%;display:block">

</div>

<div id="after-sponsor">

</div>

</div>

<div class="css-186x18t">

</div>

<div class="css-1vkm6nb ehdk2mb0">

# Many Facial-Recognition Systems Are Biased, Says U.S. Study

</div>

Algorithms falsely identified African-American and Asian faces 10 to 100
times more than Caucasian faces, researchers for the National Institute
of Standards and Technology found.

<div class="css-79elbk" data-testid="photoviewer-wrapper">

<div class="css-z3e15g" data-testid="photoviewer-wrapper-hidden">

</div>

<div class="css-1a48zt4 ehw59r15" data-testid="photoviewer-children">

![<span class="css-16f3y1r e13ogyst0" data-aria-hidden="true">Morning at
Grand Central Terminal. Technology for facial recognition is frequently
biased, a new study
confirmed.</span><span class="css-cnj6d5 e1z0qqy90" itemprop="copyrightHolder"><span class="css-1ly73wi e1tej78p0">Credit...</span><span><span>Timothy
A. Clary/Agence France-Presse — Getty
Images</span></span></span>](https://static01.nyt.com/images/2019/12/19/business/19facial/19facial-articleLarge.jpg?quality=75&auto=webp&disable=upscale)

</div>

</div>

<div class="css-18e8msd">

<div class="css-vp77d3 epjyd6m0">

<div class="css-1baulvz">

By [<span class="css-1baulvz" itemprop="name">Natasha
Singer</span>](https://www.nytimes.com/by/natasha-singer) and
[<span class="css-1baulvz last-byline" itemprop="name">Cade
Metz</span>](https://www.nytimes.com/by/cade-metz)

</div>

</div>

  - 
    
    <div class="css-ld3wwf e16638kd2">
    
    Dec. 19,
    2019
    
    </div>

  - 
    
    <div class="css-4xjgmj">
    
    <div class="css-d8bdto" data-role="toolbar" data-aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count" data-testid="share-tools">
    
      - 
      - 
      - 
      - 
        
        <div class="css-6n7j50">
        
        </div>
    
      - 
    
    </div>
    
    </div>

</div>

</div>

<div class="section meteredContent css-1r7ky0e" name="articleBody" itemprop="articleBody">

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

The majority of commercial facial-recognition systems exhibit bias,
according to a study from a federal agency released on Thursday,
underscoring questions about a technology increasingly used by police
departments and federal agencies to identify suspected criminals.

The systems [falsely identified
African-American](https://www.nytimes.com/2019/07/08/us/detroit-facial-recognition-cameras.html)
and Asian faces 10 times to 100 times more than Caucasian faces, the
National Institute of Standards and Technology reported on Thursday.
Among a database of photos used by law enforcement agencies in the
United States, the highest error rates came in identifying Native
Americans, the study found.

The technology also had more difficulty identifying women than men. And
it falsely identified older adults up to 10 times more than middle-aged
adults.

The new report comes at a time of mounting concern from lawmakers and
civil rights groups over the proliferation of facial recognition.
Proponents view it as an important tool for catching criminals and
tracking terrorists. Tech companies market it as a convenience that can
be used to help identify people in photos or in lieu of a password to
unlock smartphones.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Civil liberties experts, however, warn that the technology — which can
be used to track people at a distance without their knowledge — has the
potential to lead to ubiquitous surveillance, chilling freedom of
movement and speech. This year, San Francisco, Oakland and Berkeley in
California and the Massachusetts communities Somerville and Brookline
banned government use of the technology.

“One false match can lead to missed flights, lengthy interrogations,
watch list placements, tense police encounters, false arrests or worse,”
Jay Stanley, a policy analyst at the American Civil Liberties Union,
said in a statement. “Government agencies including the F.B.I., Customs
and Border Protection and local law enforcement must immediately halt
the deployment of this dystopian technology.”

The federal report is one of the largest studies of its kind. The
researchers had access to more than 18 million photos of about 8.5
million people from United States mug shots, visa applications and
border-crossing databases.

The National Institute of Standards and Technology tested 189
[facial-recognition
algorithms](https://www.nytimes.com/2019/01/24/technology/amazon-facial-technology-study.html)
from 99 developers, representing the majority of commercial developers.
They included systems from Microsoft, biometric technology companies
like Cognitec, and Megvii, an artificial intelligence company in China.

The agency did not test systems from Amazon, Apple, Facebook and Google
because they did not submit their algorithms for the federal study.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

The federal report confirms earlier studies from M.I.T. that reported
that [facial-recognition
systems](https://www.nytimes.com/2019/05/15/business/facial-recognition-software-controversy.html)
from some large tech companies had much lower accuracy rates in
identifying the female and darker-skinned faces [than the white male
faces](https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html).

“While some biometric researchers and vendors have attempted to claim
algorithmic bias is not an issue or has been overcome, this study
provides a comprehensive rebuttal,” Joy Buolamwini, a researcher at the
M.I.T. Media Lab who led one of the facial studies, said in an email.
“We must safeguard the public interest and halt the proliferation of
face surveillance.”

Although the use of facial recognition by law enforcement is not new,
new uses are proliferating with little independent oversight or public
scrutiny. China has used the technology to surveil and control ethnic
minority groups like the Uighurs. This year, United States Immigration
and Customs Enforcement officials came under fire for using the
technology to analyze the drivers’ licenses of millions of people
without their knowledge.

Biased facial recognition technology is particularly problematic in law
enforcement because errors could lead to false accusations and arrests.
The new federal study found that the kind of facial matching algorithms
used in law enforcement had the highest error rates for African-American
females.

“The consequences could be significant,” said Patrick Grother, a
computer scientist at N.I.S.T. who was the primary author of the new
report. He said he hoped it would spur people who develop facial
recognition algorithms to “look at the problems they may have and how
they might fix it.”

But ensuring that these systems are fair is only part of the task, said
Maria De-Arteaga, a researcher at Carnegie Mellon University who
specializes in algorithmic systems. As facial recognition becomes more
powerful, she said, companies and governments must be careful about
when, where, and how they are deployed.

“We have to think about whether we really want these technologies in our
society,” she said.

</div>

</div>

</div>

<div>

</div>

<div>

</div>

<div>

</div>

<div>

<div id="bottom-wrapper" class="css-1ede5it">

<div id="bottom-slug" class="css-l9onyx">

Advertisement

</div>

[Continue reading the main
story](#after-bottom)

<div id="bottom" class="ad bottom-wrapper" style="text-align:center;height:100%;display:block;min-height:90px">

</div>

<div id="after-bottom">

</div>

</div>

</div>

</div>

</div>

## Site Index

<div>

</div>

## Site Information Navigation

  - [© <span>2020</span> <span>The New York Times
    Company</span>](https://help.nytimes.com/hc/en-us/articles/115014792127-Copyright-notice)

<!-- end list -->

  - [NYTCo](https://www.nytco.com/)
  - [Contact
    Us](https://help.nytimes.com/hc/en-us/articles/115015385887-Contact-Us)
  - [Work with us](https://www.nytco.com/careers/)
  - [Advertise](https://nytmediakit.com/)
  - [T Brand Studio](http://www.tbrandstudio.com/)
  - [Your Ad
    Choices](https://www.nytimes.com/privacy/cookie-policy#how-do-i-manage-trackers)
  - [Privacy](https://www.nytimes.com/privacy)
  - [Terms of
    Service](https://help.nytimes.com/hc/en-us/articles/115014893428-Terms-of-service)
  - [Terms of
    Sale](https://help.nytimes.com/hc/en-us/articles/115014893968-Terms-of-sale)
  - [Site
    Map](https://spiderbites.nytimes.com)
  - [Help](https://help.nytimes.com/hc/en-us)
  - [Subscriptions](https://www.nytimes.com/subscription?campaignId=37WXW)

</div>

</div>

</div>

</div>
