<div id="app">

<div>

<div>

<div>

<div class="NYTAppHideMasthead css-1q2w90k e1suatyy0">

<div class="section css-ui9rw0 e1suatyy2">

<div class="css-eph4ug er09x8g0">

<div class="css-6n7j50">

</div>

<span class="css-1dv1kvn">Sections</span>

<div class="css-10488qs">

<span class="css-1dv1kvn">SEARCH</span>

</div>

[Skip to content](#site-content)[Skip to site
index](#site-index)

</div>

<div id="masthead-section-label" class="css-1wr3we4 eaxe0e00">

[Negocios](https://www.nytimes.com/es/section/negocios)

</div>

<div class="css-10698na e1huz5gh0">

</div>

</div>

<div id="masthead-bar-one" class="section hasLinks css-15hmgas e1csuq9d3">

<div class="css-uqyvli e1csuq9d0">

</div>

<div class="css-1uqjmks e1csuq9d1">

</div>

<div class="css-9e9ivx">

[](https://myaccount.nytimes.com/auth/login?response_type=cookie&client_id=vi)

</div>

<div class="css-1bvtpon e1csuq9d2">

[Today’s
Paper](https://www.nytimes.com/section/todayspaper)

</div>

</div>

</div>

</div>

<div data-aria-hidden="false">

<div id="site-content" data-role="main">

<div>

<div class="css-1aor85t" style="opacity:0.000000001;z-index:-1;visibility:hidden">

<div class="css-1hqnpie">

<div class="css-epjblv">

<span class="css-17xtcya">[Negocios](/es/section/negocios)</span><span class="css-x15j1o">|</span><span class="css-fwqvlz">Una
aplicación de reconocimiento facial ha identificado a víctimas de abuso
infantil</span>

</div>

<div class="css-k008qs">

<div class="css-1iwv8en">

<span class="css-18z7m18"></span>

<div>

</div>

</div>

<span class="css-1n6z4y">https://nyti.ms/37blxw9</span>

<div class="css-1705lsu">

<div class="css-4xjgmj">

<div class="css-4skfbu" data-role="toolbar" data-aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count" data-testid="share-tools">

  - 
  - 
  - 
  - 
    
    <div class="css-6n7j50">
    
    </div>

  - 

</div>

</div>

</div>

</div>

</div>

</div>

<div id="NYT_TOP_BANNER_REGION" class="css-13pd83m">

</div>

<div id="top-wrapper" class="css-1sy8kpn">

<div id="top-slug" class="css-l9onyx">

Advertisement

</div>

[Continue reading the main
story](#after-top)

<div class="ad top-wrapper" style="text-align:center;height:100%;display:block;min-height:250px">

<div id="top" class="place-ad" data-position="top" data-size-key="top">

</div>

</div>

<div id="after-top">

</div>

</div>

<div>

<div id="sponsor-wrapper" class="css-1hyfx7x">

<div id="sponsor-slug" class="css-19vbshk">

Supported by

</div>

[Continue reading the main
story](#after-sponsor)

<div id="sponsor" class="ad sponsor-wrapper" style="text-align:center;height:100%;display:block">

</div>

<div id="after-sponsor">

</div>

</div>

<div class="css-186x18t">

</div>

<div class="css-1vkm6nb ehdk2mb0">

# Una aplicación de reconocimiento facial ha identificado a víctimas de abuso infantil

</div>

Aunque la herramienta podría ayudar a resolver casos, esa tecnología
podría permitir que la hermética empresa Clearview recopile datos e
imágenes extraordinariamente sensibles.

<div class="css-79elbk" data-testid="photoviewer-wrapper">

<div class="css-z3e15g" data-testid="photoviewer-wrapper-hidden">

</div>

<div class="css-1a48zt4 ehw59r15" data-testid="photoviewer-children">

![<span class="css-16f3y1r e13ogyst0" data-aria-hidden="true">Hoan
Ton-That, el fundador de Clearview, probó su aplicación en enero de
2020.</span><span class="css-cnj6d5 e1z0qqy90" itemprop="copyrightHolder"><span class="css-1ly73wi e1tej78p0">Credit...</span><span><span>Amr
Alfiky para The New York
Times</span></span></span>](https://static01.nyt.com/images/2020/02/07/business/10ClearviewES-1/merlin_167287035_0c3ff0e2-b4b7-4c2b-a1a7-e5054b500409-articleLarge.jpg?quality=75&auto=webp&disable=upscale)

</div>

</div>

<div class="css-18e8msd">

<div class="css-vp77d3 epjyd6m0">

<div class="css-1baulvz">

Por [<span class="css-1baulvz" itemprop="name">Kashmir
Hill</span>](https://www.nytimes.com/by/kashmir-hill) y
[<span class="css-1baulvz last-byline" itemprop="name">Gabriel J.X.
Dance</span>](https://www.nytimes.com/by/gabriel-dance)

</div>

</div>

  - 10 de febrero de
    2020

  - 
    
    <div class="css-4xjgmj">
    
    <div class="css-d8bdto" data-role="toolbar" data-aria-label="Social Media Share buttons, Save button, and Comments Panel with current comment count" data-testid="share-tools">
    
      - 
      - 
      - 
      - 
        
        <div class="css-6n7j50">
        
        </div>
    
      - 
    
    </div>
    
    </div>

</div>

<div class="css-mdjrty">

[Read in
English](https://www.nytimes.com/2020/02/07/business/clearview-facial-recognition-child-sexual-abuse.html "Read in English")

</div>

</div>

<div class="section meteredContent css-1r7ky0e" name="articleBody" itemprop="articleBody">

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Agencias de seguridad de Estados Unidos y Canadá están usando Clearview
AI —una hermética empresa que usa tecnología de reconocimiento facial a
partir de una base de datos de 3000 millones de imágenes— para
identificar a niños que son víctimas de abuso sexual. Es un poderoso
incentivo para usar la tecnología, pero genera nuevos cuestionamientos
sobre la precisión de esa herramienta y el manejo de los datos que hace
la empresa.

Según los investigadores, las herramientas de Clearview les permiten
saber los nombres o el paradero de menores que aparecen en videos o
fotos de explotación sexual y que, de otro modo, tal vez no habrían sido
identificados. En un caso en Indiana, los detectives pasaron imágenes de
21 víctimas del mismo delincuente por la aplicación de Clearview y
recibieron catorce identidades, de acuerdo con Charles Cohen, un jefe
retirado de la policía estatal. La víctima más joven tenía 13 años.

</div>

</div>

<div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

“Eran chicos o mujeres jóvenes. Queríamos encontrarlos para decirles que
habíamos arrestado a este tipo y ver si querían declarar en calidad de
víctimas”, señaló Cohen.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Otro funcionario, uno de los responsables de la identificación de
víctimas en Canadá, quien no estaba autorizado para hablar en público
sobre las investigaciones, describió la tecnología de Clearview como “el
avance más importante en la última década” en el terreno de los delitos
de abuso sexual infantil.

Sin embargo, según defensores de la privacidad, no se ha probado ni
regulado la base de datos de la empresa, la cual podría causar nuevos
tipos de daños. En sus servidores, Clearview almacena fotos que suben
los investigadores —conocidas como “imágenes de sondas”—, es decir que
podría acumular un conjunto de datos de una delicadeza extraordinaria
sobre víctimas infantiles de abuso sexual y explotación.

“Comprendemos la sensibilidad extrema involucrada con la identificación
de niños”, escribió en un correo electrónico Hoan Ton-That, fundador de
Clearview. “Nuestra misión es proteger a los menores”.

De acuerdo con un
[documento](https://int.nyt.com/data/documenthelper/6690-clearview-faq/c8b081a0bcca12e7903a/optimized/full.pdf#page=1)
que la empresa comparte con sus clientes, “las búsquedas se guardan para
siempre” por omisión, pero los administradores pueden cambiar sus
configuraciones para que las imágenes de esas búsquedas sean purgadas
después de 30 días.

En su mayor parte, Clearview había operado en las sombras hasta el mes
pasado, cuando la publicación de un [reportaje de The New York
Times](https://www.nytimes.com/es/2020/01/20/espanol/negocios/clearview-reconocimiento-facial.html)
reveló el uso que le daban agencias de seguridad a nivel local y federal
de todo Estados Unidos. La empresa ha recolectado miles de millones de
fotos de individuos de todo el internet público, en sitios como
Facebook, Twitter, Venmo y YouTube. Cuando un usuario sube la foto de
una persona a Clearview, la aplicación muestra otras imágenes de la
persona y las direcciones web donde aparece.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

En numerosos documentos publicitarios, Clearview promueve el uso que le
dan las agencias de seguridad a su tecnología para resolver casos de
abuso sexual infantil. No obstante, hasta hace poco, la empresa se
centraba en su participación para identificar a los perpetradores, no a
las víctimas.

Quienes han alertado sobre los riesgos de Clearview han argumentado que
los beneficios de ese tipo de base de datos no son mayores que los daños
que podían causar.

“Es difícil. Todo el mundo quiere seguridad y salvar a los niños”, dijo
Liz O’Sullivan, directora de tecnología en Surveillance Technology
Oversight Project. “Siempre hay alguna manera de normalizar la
vigilancia, pero sería peligroso que nos enfocáramos en las ventajas
potenciales. El reconocimiento facial comete muchos errores”.

O’Sullivan mencionó que le preocupaba que una agencia independiente no
hubiera probado la precisión del software de Clearview. Los algoritmos
de reconocimiento facial pueden ser deficientes en la gente joven, en
parte porque sus rostros cambian con la edad y también porque los niños
a menudo no están incluidos en los archivos de datos que se usan para
entrenar a los algoritmos.

O’Sullivan advirtió que, si la herramienta se equivoca en una
coincidencia, podría provocar efectos devastadores para los niños que
hayan sido identificados de forma errónea y para sus familias.
“Intercambiar la libertad y la privacidad por alguna evidencia
anecdótica que pudiera servirles a ciertas personas nunca es suficiente
para ceder nuestras libertades civiles”, opinó O’Sullivan.

Las agencias de seguridad deben verificar cada identidad cuando usan la
aplicación de Clearview, comentó Ton-That en su correo electrónico. Sin
embargo, no supo decir cuántos niños había en su base de datos.

“No monitoreamos el desglose de edad, género o raza de nuestra base de
datos de imágenes”, comentó. “Somos un motor de búsqueda de imágenes
públicas, no un sistema de vigilancia”.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Las fuerzas especiales de Florida, Indiana y Dakota del Sur dedicadas a
investigar casos de abuso infantil, así como el Departamento de
Seguridad Nacional estadounidense y las agencias de seguridad de Canadá,
utilizan la aplicación.

Al igual que varios funcionarios que declararon para el Times, el
investigador canadiense fue reacio a hablar sobre Clearview por temor a
que los delincuentes cambiaran sus tácticas. “Nos preocupa que, cuando
los criminales sepan que está disponible, cubran más el rostro de sus
víctimas”, comentó el oficial. “No queremos que ellos sepan que se
puede hacer esto”.

</div>

</div>

<div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Hay riesgos legales asociados con el manejo de este tipo de imágenes. La
empresa violaría la ley si recibe imágenes de abuso y no informa de
inmediato a las autoridades y borra el material de sus servidores.
Ton-That mencionó que la aplicación de Clearview solo transmite rostros,
no imágenes completas.

El Times verificó esto al analizar una versión para Android de la
aplicación de Clearview, pero no pudo examinar la oferta de la empresa
para iOS ni una versión para web.

Ninguna de las agencias de seguridad con las que habló el Times mencionó
si había realizado una auditoría técnica a Clearview antes de usar el
software. Tampoco respondieron a las preguntas relacionadas con el uso
específico de la aplicación, al argumentar que no comentaban sobre sus
técnicas de investigación.

Britney Walker, una vocera de la Unidad de Investigación de Explotación
Infantil del Departamento de Seguridad Nacional de Estados Unidos,
mencionó que la división colabora con otras agencias para que le ayuden
con las investigaciones, pero que la estrategia “centrada en las
víctimas” que utiliza la unidad prohíbe compartir imágenes ilegales.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

“La agencia, en ninguna circunstancia, compartiría material de abuso
sexual infantil a empresas privadas”, comentó Walker.

Otras compañías ya trabajan de cerca con funcionarios de agencias de
seguridad para investigar casos de abuso sexual infantil. Johann
Hofmann, director ejecutivo de Griffeye, señaló que el software de la
empresa que analiza las imágenes fue instalado dentro de las redes de
las agencias de seguridad y estaba diseñado para evitar el envío de
imágenes a terceros, incluida la misma Griffeye.

Otra empresa que ofrece herramientas analíticas a los investigadores de
abuso sexual infantil, CameraForensics, también comentó que sus sistemas
estaban diseñados para nunca recibir ningún tipo de imágenes de agencias
de seguridad, incluidos rostros. El fundador de la empresa, Matt Burns,
mencionó que su firma había considerado incorporar la tecnología de
reconocimiento facial en su software, pero había decidido no hacerlo por
“razones éticas”.

“Consideramos que era una herramienta demasiado controversial porque es
muy fácil caer en casos de abuso con esa funcionalidad”, dijo. “Y
también es una pesadilla legal”.

Pero Burns entiende por qué los investigadores querrían usar un software
de reconocimiento facial. “Se enfrentan a una tarea muy sombría y, si
hay una herramienta que les da la posibilidad de proteger a las
víctimas, no los culpo por usarla”, dijo.

Desde que salieron a la luz las prácticas de Clearview, compañías como
Facebook, LinkedIn, Twitter, Venmo y YouTube han
[enviado](https://www.nytimes.com/2020/01/22/technology/clearview-ai-twitter-letter.html)
cartas en las que le solicitan a la empresa dejar de tomar fotos de sus
sitios y borrar las imágenes existentes de sus bases de datos. El fiscal
general de Nueva Jersey les
[prohibió](https://www.nytimes.com/2020/01/24/technology/clearview-ai-new-jersey.html)
el uso de Clearview a los funcionarios del estado y solicitó una
investigación sobre el uso que les daban las agencias de seguridad a la
empresa y a las tecnologías similares. En Illinois, donde una dura ley
de privacidad de los datos biométricos prohíbe el uso de los rostros de
los habitantes sin su consentimiento, se presentó una demanda colectiva
en busca de certificación. El 3 de febrero, se presentó otra en
Virginia.

Recientemente, en Nueva York y Washington, se han presentado proyectos
de ley que le prohíben el uso del reconocimiento facial a la policía.
Además, Clearview recibió una carta de Edward Markey, senador demócrata
de Massachusetts, en la que le pidió una lista de las agencias de
seguridad que han usado la aplicación y si se ha recabado la información
biométrica de niños menores de 13 años.

</div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

“Aunque este tipo de tecnología existe desde hace bastante tiempo,
creemos que hemos creado algo que permite a las fuerzas del orden
público resolver crímenes que antes no se podían resolver y, lo más
importante, proteger a menores vulnerables”, dijo Ton-That en su correo
electrónico. “Al mismo tiempo, estamos respondiendo a las solicitudes de
información del gobierno estadounidense y otras partes interesadas,
según corresponda, y esperamos entablar conversaciones constructivas
con ellos mientras trabajamos para hacer que nuestras comunidades sean
más seguras”.

En octubre, grupos de agencias policiales enviaron [una
carta](https://www.ascia.org/pdf/news/le_group_letter_to_congress__facial_recogniton_technology__october_2019.pdf)
a los miembros del congreso de Estados Unidos en la que hacen un llamado
a no prohibir el uso del reconocimiento facial para sus investigaciones.
“Entendemos la preocupación del público sobre los derechos civiles y la
protección de su privacidad”, escribieron. “Con políticas claras y
disponibles al público, creemos que esas inquietudes pueden abordarse”.

Muchas agencias ya habían estado usando Clearview durante meses, pero la
carta no mencionaba eso.

Michael H. Keller y Aaron Krolik colaboraron en este reportaje.

</div>

</div>

<div>

</div>

<div class="css-1fanzo5 StoryBodyCompanionColumn">

<div class="css-53u6y8">

Kashmir Hill es una reportera de tecnología radicada en Nueva York.
Escribe sobre las formas inesperadas en las que la tecnología está
cambiando nuestras vidas, particularmente cuando se trata de nuestra
privacidad. [@kashhill](https://twitter.com/kashhill)

Gabriel Dance es el editor adjunto de investigaciones. Antes fue editor
interactivo del diario The Guardian y formó parte del equipo que recibió
el Pulitzer en 2014 por la cobertura de la vigilancia secreta por parte
de la Agencia de Seguridad Nacional de Estados Unidos.
[@gabrieldance](https://twitter.com/gabrieldance)

-----

</div>

</div>

</div>

<div>

</div>

<div>

</div>

<div>

</div>

<div>

<div id="bottom-wrapper" class="css-1ede5it">

<div id="bottom-slug" class="css-l9onyx">

Advertisement

</div>

[Continue reading the main
story](#after-bottom)

<div id="bottom" class="ad bottom-wrapper" style="text-align:center;height:100%;display:block;min-height:90px">

</div>

<div id="after-bottom">

</div>

</div>

</div>

</div>

</div>

## Site Index

<div>

</div>

## Site Information Navigation

  - [© <span>2020</span> <span>The New York Times
    Company</span>](https://help.nytimes.com/hc/en-us/articles/115014792127-Copyright-notice)

<!-- end list -->

  - [NYTCo](https://www.nytco.com/)
  - [Contact
    Us](https://help.nytimes.com/hc/en-us/articles/115015385887-Contact-Us)
  - [Work with us](https://www.nytco.com/careers/)
  - [Advertise](https://nytmediakit.com/)
  - [T Brand Studio](http://www.tbrandstudio.com/)
  - [Your Ad
    Choices](https://www.nytimes.com/privacy/cookie-policy#how-do-i-manage-trackers)
  - [Privacy](https://www.nytimes.com/privacy)
  - [Terms of
    Service](https://help.nytimes.com/hc/en-us/articles/115014893428-Terms-of-service)
  - [Terms of
    Sale](https://help.nytimes.com/hc/en-us/articles/115014893968-Terms-of-sale)
  - [Site
    Map](https://spiderbites.nytimes.com)
  - [Help](https://help.nytimes.com/hc/en-us)
  - [Subscriptions](https://www.nytimes.com/subscription?campaignId=37WXW)

</div>

</div>

</div>

</div>
